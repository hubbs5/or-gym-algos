{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the Knapsack Problem\n",
    "\n",
    "The Knapsack Problem (KP) is a combinatorial optimization problem which requires the user to select from a range of goods of different values and weights in order to maximize the value of the selected items within a given weight limit. This version is unbounded meaning the we can select items without limit. \n",
    "\n",
    "The episodes proceed by selecting items and placing them into the knapsack one at a time until the weight limit is reached or exceeded, at which point the episode ends.\n",
    "\n",
    "Here, we'll be solving the unbounded version of the problem meaning there is no limit to the number of times we can select any of the items. This version is incredibly small and simply used to confirm the validity of the models and solution mechanisms. We'll be optimizing using two methods, dynamic programming and mathematical programming to demonstrate both methodologies and build confidence in the solutions and environment. From here, we can then expand the environment to larger and more complex versions of the problem involving uncertainty, multiple knapsacks, limited items, etc.\n",
    "\n",
    "## Knapsack Environment\n",
    "\n",
    "If you have installed the `or-gym` package and OpenAI gym, you can import the modules and build the environment as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import or_gym\n",
    "\n",
    "env = gym.make('Knapsack-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated above, this is a very limited version of the problem with only a handful of items, numbered 1 to N. Each of these items has a corresponding weight and a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item\t0\tWeight\t1\tValue\t0\n",
      "Item\t1\tWeight\t2\tValue\t1\n",
      "Item\t2\tWeight\t3\tValue\t3\n",
      "Item\t3\tWeight\t6\tValue\t14\n",
      "Item\t4\tWeight\t10\tValue\t20\n",
      "Item\t5\tWeight\t18\tValue\t100\n"
     ]
    }
   ],
   "source": [
    "for i in env.item_numbers:\n",
    "    print('Item\\t{}\\tWeight\\t{}\\tValue\\t{}'.format(\n",
    "        i, env.item_weights[i], env.item_values[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total weight limit for this problem is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.max_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state is defined as a tuple with three entries: \n",
    "- Item weights\n",
    "- Item values\n",
    "- Current knapsack weight\n",
    "\n",
    "This can be accessed with the `_get_obs()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve KP with DP\n",
    "\n",
    "First, we'll solve this model using a dynamic programming algorithm called **policy iteration**.\n",
    "\n",
    "Thikning about this carefully, we see that, for the simple dynamic programming approach to the unbounded problem, there are only as many states as the carrying capacity divided by the smallest weight plus one for anything greater than the maximum capacity. In the simple case above, we have a max capacity of 15, and the smallest item we have has a weight of 1. This gives us 16 states. This becomes much more complicated when we have a bounded problem such that the state is defined by the weight and the remaining number of items to pack, so we'll leave that for later.\n",
    "\n",
    "Policy iteration solves the problem in two steps, first evaluating an initial policy, then updating that policy based on the improved value estimate. The algorithm terminates when there are no longer meaningful changes to the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, gamma):\n",
    "    # Value iteration\n",
    "    v = np.zeros(env.max_weight + 1)\n",
    "\n",
    "    # Terminal state values are 0\n",
    "    delta = 1e-5\n",
    "    delta_t = 1\n",
    "    k = 0\n",
    "    while delta_t > delta:\n",
    "        v_new = np.zeros(v.shape)\n",
    "        for i in range(v_new.shape[0]):\n",
    "            # Check for terminal state\n",
    "            if i == env.max_weight:\n",
    "                continue\n",
    "            else:\n",
    "                for action in env.item_numbers:\n",
    "                    env.current_weight = i\n",
    "                    env._get_obs()\n",
    "                    s_1, r, done, _ = env.step(action)\n",
    "                    prob = policy[i, action]\n",
    "                    v_new[i] += prob * (r + gamma * v[s_1[-1]])\n",
    "\n",
    "        k += 1\n",
    "        delta_t = np.sum(np.abs(v - v_new))\n",
    "        v = v_new.copy()\n",
    "    return v\n",
    "\n",
    "def policy_improvement(env, policy, values, gamma):\n",
    "    new_policy = np.zeros(policy.shape)\n",
    "    \n",
    "    for i in range(new_policy.shape[0]):\n",
    "        for j in range(new_policy.shape[1]):\n",
    "            # Get the action values\n",
    "            action_vals = []\n",
    "            for action in env.item_numbers:\n",
    "                # Update state and take step\n",
    "                env.current_weight = i\n",
    "                env._get_obs()\n",
    "                s_1, r, done, _ = env.step(action)\n",
    "                val = r + gamma * (values[i])\n",
    "                action_vals.append([val, action])\n",
    "                \n",
    "            # Select the best action\n",
    "            action_vals = np.array(action_vals)\n",
    "            best_actions = np.argwhere(\n",
    "                action_vals[:,0]==np.max(action_vals[:, 0])).flatten()\n",
    "            # Randomize in cases where multiple maximums exist\n",
    "            if len(best_actions) > 1:\n",
    "                best_actions = np.random.choice(best_actions)\n",
    "            new_policy[i, best_actions] = 1\n",
    "\n",
    "    num_policy_changes = np.sum(policy != new_policy).astype(int)\n",
    "    return new_policy, num_policy_changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we have a function called `play_policy` that applies the current policy directly to the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_policy(env, policy):\n",
    "    state = env.reset()[-1]\n",
    "    done = False\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    while done == False:\n",
    "        action = np.argmax(policy[state])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        state = next_state[-1]\n",
    "    return np.array(rewards), np.array(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we initialize our policy assuming equal probability of all actions. Then we continue to alternate between policy evaluation and improvement until meaningful changes cease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 3 iterations\n"
     ]
    }
   ],
   "source": [
    "# Initialize Policy\n",
    "policy = np.ones((env.max_weight + 1, env.N)) * 1 / env.N\n",
    "env.reset()\n",
    "gamma = 1\n",
    "k = 0\n",
    "reward_deque = deque(maxlen=3)\n",
    "action_deque = deque(maxlen=3)\n",
    "while True:\n",
    "    values = policy_evaluation(env, policy, gamma)\n",
    "    policy, num_changes = policy_improvement(env, policy, values, gamma)\n",
    "    k += 1\n",
    "    if num_changes == 0:\n",
    "        break\n",
    "    # The policy changes cease to matter after a certain point swapping a few\n",
    "    # high-weight items. We'll evaluate it directly on the environment and\n",
    "    # and stop if 3 successive policies yield the exact same actions and rewards.\n",
    "    rewards, actions = play_policy(env, policy)\n",
    "    reward_deque.append(rewards)\n",
    "    action_deque.append(actions)\n",
    "    if len(reward_deque) == 3:\n",
    "        converge_r = all([np.allclose(reward_deque[i-1], reward_deque[i])\n",
    "            for i, j in enumerate(reward_deque)])\n",
    "        converge_a = all([np.allclose(action_deque[i-1], action_deque[i])\n",
    "            for i, j in enumerate(action_deque)])\n",
    "        if converge_r and converge_a:\n",
    "            print(\"Converged after {} iterations\".format(k))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The small problem enables the DP solution to converge quickly. The results are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items selected:\t[4 2 1]\n",
      "Total reward:\t24\n"
     ]
    }
   ],
   "source": [
    "print(\"Items selected:\\t{}\\nTotal reward:\\t{}\".format(actions, rewards.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Programming\n",
    "\n",
    "This model may also be solved via mathematical programming whereby we can construct an integer program according to the constraints given below.\n",
    "\n",
    "$$\\textrm{max} \\; z = \\sum_i v_i x_i$$\n",
    "\n",
    "$$\\textrm{s.t.} \\; \\sum_i w_i x_i \\leq W$$\n",
    "\n",
    "$$x_i \\in [0, 1]$$\n",
    "\n",
    "$$v_i, w_i \\in \\rm I\\!R^+$$\n",
    "\n",
    "- $i$ indexes the items\n",
    "- $v_i$ is the value of each item $i$\n",
    "- $w_i$ is the weight of each item $i$\n",
    "- $x_i$ is a binary decision variable; 1 denotes a selection of the given item for the knapsack.\n",
    "- $W$ is the weight limit.\n",
    "\n",
    "We'll optimize this model using Pyomo 5.6 and the GLPK solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyomo.environ import *\n",
    "from pyomo.opt import SolverFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "m = ConcreteModel()\n",
    "\n",
    "# Sets, parameters, and variables\n",
    "m.W = env.max_weight\n",
    "m.i = Set(initialize=env.item_numbers)\n",
    "m.w = Param(m.i, \n",
    "    initialize={i: j for i, j in zip(env.item_numbers, env.item_weights)})\n",
    "m.v = Param(m.i, \n",
    "    initialize={i: j for i, j in zip(env.item_numbers, env.item_values)})\n",
    "m.x = Var(m.i, within=NonNegativeIntegers)\n",
    "\n",
    "# Define contstraints\n",
    "@m.Constraint(m.i)\n",
    "def weight_constraint(m, i):\n",
    "    return sum(m.w[i] * m.x[i] for i in m.i) - m.W <= 0\n",
    "\n",
    "m.obj = Objective(expr=(\n",
    "    sum([m.v[i] * m.x[i] for i in m.i])),\n",
    "    sense=maximize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLPSOL: GLPK LP/MIP Solver, v4.57\n",
      "Parameter(s) specified in the command line:\n",
      " --write /tmp/tmp2epjflap.glpk.raw --wglp /tmp/tmp5hasqvq_.glpk.glp --cpxlp\n",
      " /tmp/tmpxisnfude.pyomo.lp\n",
      "Reading problem data from '/tmp/tmpxisnfude.pyomo.lp'...\n",
      "7 rows, 7 columns, 37 non-zeros\n",
      "6 integer variables, none of which are binary\n",
      "84 lines were read\n",
      "Writing problem data to '/tmp/tmp5hasqvq_.glpk.glp'...\n",
      "73 lines were written\n",
      "GLPK Integer Optimizer, v4.57\n",
      "7 rows, 7 columns, 37 non-zeros\n",
      "6 integer variables, none of which are binary\n",
      "Preprocessing...\n",
      "6 rows, 5 columns, 30 non-zeros\n",
      "5 integer variables, one of which is binary\n",
      "Scaling...\n",
      " A: min|aij| =  1.000e+00  max|aij| =  1.000e+01  ratio =  1.000e+01\n",
      "Problem data seem to be well scaled\n",
      "Constructing initial basis...\n",
      "Size of triangular part is 6\n",
      "Solving LP relaxation...\n",
      "GLPK Simplex Optimizer, v4.57\n",
      "6 rows, 5 columns, 30 non-zeros\n",
      "*     0: obj =  -0.000000000e+00 inf =   0.000e+00 (4)\n",
      "*     3: obj =   3.400000000e+01 inf =   0.000e+00 (0)\n",
      "OPTIMAL LP SOLUTION FOUND\n",
      "Integer optimization begins...\n",
      "+     3: mip =     not found yet <=              +inf        (1; 0)\n",
      "Solution found by heuristic: 28\n",
      "+     4: >>>>>   3.100000000e+01 <=   3.100000000e+01   0.0% (2; 0)\n",
      "+     4: mip =   3.100000000e+01 <=     tree is empty   0.0% (0; 3)\n",
      "INTEGER OPTIMAL SOLUTION FOUND\n",
      "Time used:   0.0 secs\n",
      "Memory used: 0.1 Mb (53775 bytes)\n",
      "Writing MIP solution to '/tmp/tmp2epjflap.glpk.raw'...\n",
      "16 lines were written\n"
     ]
    }
   ],
   "source": [
    "# Solve model\n",
    "solver = SolverFactory('glpk')\n",
    "results = solver.solve(m, tee=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items selected:\t[2]\n",
      "Total reward:\t31.0\n"
     ]
    }
   ],
   "source": [
    "ip_selections = [i for i in m.i if m.x[i]==1]\n",
    "print(\"Items selected:\\t{}\\nTotal reward:\\t{}\".format(\n",
    "    ip_selections, m.obj.expr()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the selections and the total rewards are identical for both the DP and IP approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UKP Heuristic Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ukp_heuristic(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    # Get value-weight ratios\n",
    "    vw_ratio = env.item_values / env.item_weights\n",
    "    vw_order = env.item_numbers[np.argsort(vw_ratio)[::-1]]\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    while not done:\n",
    "        max_item = vw_order[0]\n",
    "        # Check that item fits\n",
    "        if env.item_weights[max_item] > (env.max_weight - env.current_weight):\n",
    "            # Remove item from list\n",
    "            vw_order = vw_order[1:].copy()\n",
    "            continue\n",
    "        # Select max_item\n",
    "        state, reward, done, _ = env.step(max_item)\n",
    "        actions.append(max_item)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "    return actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions, rewards = ukp_heuristic('Knapsack-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
