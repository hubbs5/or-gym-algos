{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example from [Grid Dynamics](https://blog.griddynamics.com/deep-reinforcement-learning-for-supply-chain-and-price-optimization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import pandas as pd\n",
    "import collections\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from qbstyles import mpl_style\n",
    "import gym\n",
    "from gym import spaces\n",
    "%matplotlib inline\n",
    "mpl_style(dark=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20, 20, 20, 20], dtype=int16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([np.nan, np.nan, 1])\n",
    "env.action_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Multi-Echelon Environment\n",
    "class SupplyChainEnv(gym.Env):\n",
    "    def __init__(self, config):\n",
    "        \n",
    "        self.warehouse_num = 3\n",
    "        self.demand_history_len = 4\n",
    "        self.min_obs, self.max_obs = -10000, 10000\n",
    "        self.T = 26\n",
    "        self.d_max = 5\n",
    "        self.d_var = 2\n",
    "        self.unit_price = 100\n",
    "        self.unit_cost = 40\n",
    "        \n",
    "        self.storage_capacities = np.fromfunction(\n",
    "            lambda x: 10*(x+1), (self.warehouse_num + 1,), dtype=int)\n",
    "        self.storage_costs = np.fromfunction(\n",
    "            lambda x: 2*(x+1), (self.warehouse_num + 1,), dtype=int)\n",
    "        self.transportation_costs = np.fromfunction(\n",
    "            lambda x: 5*(x+1), (self.warehouse_num,), dtype=int)\n",
    "        self.penalty_unit_cost = self.unit_price\n",
    "                \n",
    "        self.reset()\n",
    "        \n",
    "        self.action_space = spaces.Box(low=0.0, high=20.0, \n",
    "            shape=(self.warehouse_num + 1,), dtype=np.int16)\n",
    "        self.observation_space = spaces.Box(self.min_obs, self.max_obs, \n",
    "            shape=(self.state.shape), dtype=np.float32)\n",
    "        \n",
    "        self.state_indices = {\n",
    "            'factory_stock': 0,\n",
    "            'warehouse_stock': np.arange(1, self.warehouse_num + 1),\n",
    "            'demand_history': np.arange(self.warehouse_num + 1, \n",
    "                self.warehouse_num *(1 + self.demand_history_len) + 1),\n",
    "            'timestep': self.warehouse_num * (1 + self.demand_history_len) + 1\n",
    "        }\n",
    "        \n",
    "    def _generate_demand(self, j, t):\n",
    "        return np.round(self.d_max/2 \\\n",
    "            + self.d_max/2*np.sin(2*np.pi*(t + 2*j) / self.T*2) \\\n",
    "            + np.random.randint(0, self.d_var))\n",
    "    \n",
    "    def _initialize_demand(self):\n",
    "        self.demand_history = collections.deque(maxlen=self.demand_history_len)\n",
    "        _ = [self.demand_history.append(np.zeros(self.warehouse_num))\n",
    "             for i in range(self.demand_history_len)]\n",
    "        \n",
    "    def generate_demand(self):\n",
    "        return np.fromfunction(lambda x: \n",
    "            self._generate_demand(x+1, self.t), \n",
    "            (self.warehouse_num,), dtype=int)\n",
    "    \n",
    "    def init_state(self):\n",
    "        self.t = 0\n",
    "        self._initialize_demand()\n",
    "        self.factory_stock = 0\n",
    "        self.warehouse_stock = np.repeat(\n",
    "            self.factory_stock, self.warehouse_num)\n",
    "    \n",
    "    def get_state(self):\n",
    "        state = np.concatenate(\n",
    "            ([self.factory_stock],\n",
    "              self.warehouse_stock,\n",
    "              np.hstack(self.demand_history),\n",
    "             [self.t]))\n",
    "        return self._check_bounds(state)\n",
    "\n",
    "    def reset(self):\n",
    "        self.init_state()\n",
    "        self.state = self.get_state()\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_stock_levels(self):\n",
    "        return np.concatenate(\n",
    "            ([self.factory_stock], self.warehouse_stock))\n",
    "    \n",
    "    def _check_bounds(self, state):\n",
    "        # Ensure state is within observation bounds\n",
    "        state = np.where(state<self.min_obs, self.min_obs, state)\n",
    "        state = np.where(state>self.max_obs, self.max_obs, state)\n",
    "        return state\n",
    "    \n",
    "    def _check_action(self, action):\n",
    "        return np.array([self.action_space.high if np.isnan(i) else i \n",
    "            for i in action])\n",
    "\n",
    "    def step(self, action, demand=None):\n",
    "        if demand is None:\n",
    "            demand = self.generate_demand()\n",
    "        action = self._check_action(action)\n",
    "        \n",
    "        total_revenue = self.unit_price * demand.sum()\n",
    "        total_production_cost = self.unit_cost * action[0]\n",
    "        total_storage_cost = np.dot(self.storage_costs, \n",
    "            np.maximum(self.get_stock_levels(),\n",
    "                np.zeros(self.warehouse_num + 1)))\n",
    "        total_penalty_cost = -self.penalty_unit_cost * (np.sum(\n",
    "            np.minimum(self.warehouse_stock, np.zeros(self.warehouse_num))) + \\\n",
    "            np.minimum(self.factory_stock, 0))\n",
    "        total_transportation_cost = np.dot(self.transportation_costs,\n",
    "            action[1:])\n",
    "        reward = total_revenue - total_production_cost - total_storage_cost - \\\n",
    "            total_penalty_cost - total_transportation_cost\n",
    "        if np.isnan(reward):\n",
    "            raise ValueError(\"NaN found in reward.\\nState = {}\\nReward = {}\".format(\n",
    "                self.state, reward)\n",
    "                + \"\\nRev = {}\\tProd Cost = {}\\tStorage Cost = {}\".format(\n",
    "                    total_revenue, total_production_cost, total_storage_cost)\n",
    "                + \"\\nPenalty Cost = {}\\tTransportation Cost = {}\".format(\n",
    "                    total_penalty_cost, total_transportation_cost)\n",
    "                + \"\\nActions = {}\".format(action))\n",
    "        if np.isinf(reward):\n",
    "            raise ValueError(\"Inf found in reward.\\nState = {}\\nReward = {}\".format(\n",
    "                self.state, reward)\n",
    "                + \"\\nRev = {}\\tProd Cost = {}\\tStorage Cost = {}\".format(\n",
    "                    total_revenue, total_production_cost, total_storage_cost)\n",
    "                + \"\\nPenalty Cost = {}\\tTransportation Cost = {}\".format(\n",
    "                    total_penalty_cost, total_transportation_cost)\n",
    "                + \"\\nActions = {}\".format(action))\n",
    "        # Update state\n",
    "        self.factory_stock = min(self.factory_stock + action[0] - np.sum(action[1:]),\n",
    "            self.storage_capacities[0])\n",
    "        \n",
    "        next_state_warehouse_stock = np.zeros(self.warehouse_stock.shape)\n",
    "        for w in range(self.warehouse_num):\n",
    "            next_state_warehouse_stock[w] = min(\n",
    "                self.warehouse_stock[w] + action[w+1] - \n",
    "                demand[w], self.storage_capacities[w+1])\n",
    "        self.warehouse_stock = next_state_warehouse_stock.copy()\n",
    "        \n",
    "        self.state = self.get_state()\n",
    "        self.t += 1\n",
    "        self.demand_history.append(demand)\n",
    "        \n",
    "        return self.state, reward, self.t == self.T - 1, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SupplyChainEnv({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n",
      "100.0\n",
      "-603536.8\n"
     ]
    }
   ],
   "source": [
    "from sc_env import SimpleSupplyChain\n",
    "\n",
    "env1 = SupplyChainEnv({})\n",
    "env2 = SimpleSupplyChain({})\n",
    "\n",
    "total_rewards = []\n",
    "ep_matching_states = []\n",
    "ep_matching_rewards = []\n",
    "for i in range(100):\n",
    "    # Ensure same outputs from different models\n",
    "    env1.reset()\n",
    "    env2.reset()\n",
    "    states, rewards = [], []\n",
    "    detailed_states, matching_rewards = [], []\n",
    "    done = False\n",
    "    count = 0\n",
    "    while done == False:\n",
    "        action = env2.action_space.sample()\n",
    "        s2, r2, done2, _ = env2.step(action)\n",
    "        demand = env2.supply_chain.demand_history[-1].copy()\n",
    "        s1, r1, done1, _ = env1.step(action, demand)\n",
    "        states.append(s1==s2)\n",
    "        detailed_states.append(np.vstack([s1, s2]).T)\n",
    "        matching_rewards.append(r1==r2)\n",
    "        rewards.append(r2)\n",
    "        count += 1\n",
    "        if done2 == True:\n",
    "            ep_matching_states.append(sum(sum(states))/(len(states)*len(states[0]))*100)\n",
    "            ep_matching_rewards.append(sum(matching_rewards)/len(matching_rewards)*100)\n",
    "            total_rewards.append(sum(rewards))\n",
    "            break\n",
    "            \n",
    "# matching_states = [all(i) for i in states]\n",
    "# print(\"Rewards match:\\t {:.1f}%\".format(sum(matching_rewards)/len(matching_rewards)*100))\n",
    "# # print(\"States match:\\t {:.1f}%\".format(sum(matching_states)/len(matching_states)*100))\n",
    "# print(\"States match:\\t {:.1f}%\".format(sum(sum(states))/(len(states) * len(states[0])) * 100))\n",
    "# print(\"State Vector Matches (%):\\n\\t{}\".format(sum(states)/len(states)*100))\n",
    "\n",
    "print(np.mean(ep_matching_states))\n",
    "print(np.mean(ep_matching_rewards))\n",
    "print(np.mean(total_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.utils import try_import_tf\n",
    "\n",
    "import ray.rllib.agents.ddpg as ddpg\n",
    "from ray.rllib import agents\n",
    "from ray.tune.logger import pretty_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-12 09:16:27,147\tWARNING services.py:597 -- setpgrp failed, processes may not be cleaned up properly: [Errno 1] Operation not permitted.\n",
      "2020-05-12 09:16:27,149\tINFO resource_spec.py:216 -- Starting Ray with 6.01 GiB memory available for workers and up to 3.01 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n"
     ]
    }
   ],
   "source": [
    "tf = try_import_tf()\n",
    "    \n",
    "# ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "def train_ddpg():\n",
    "    config = ddpg.DEFAULT_CONFIG.copy()\n",
    "    config[\"log_level\"] = \"WARN\"\n",
    "    config[\"actor_hiddens\"] = [512, 512] \n",
    "    config[\"critic_hiddens\"] = [512, 512]\n",
    "    config[\"gamma\"] = 0.95\n",
    "    config[\"timesteps_per_iteration\"] = 1000\n",
    "    config[\"target_network_update_freq\"] = 5\n",
    "    config[\"buffer_size\"] = 10000\n",
    "    \n",
    "    trainer = ddpg.DDPGTrainer(config=config, env=SupplyChainEnv)\n",
    "    for i in range(10):\n",
    "        result = trainer.train()\n",
    "        print(pretty_print(result))\n",
    "        checkpoint = trainer.save()\n",
    "        print(\"Checkpoint saved at\", checkpoint)\n",
    "    return trainer, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 18:29:23,778\tWARNING util.py:45 -- Install gputil for GPU system monitoring.\n",
      "/home/christian/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/christian/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_metrics: {}\n",
      "date: 2020-05-11_18-29-25\n",
      "done: false\n",
      "episode_len_mean: 25.0\n",
      "episode_reward_max: -545228.857820034\n",
      "episode_reward_mean: -545228.857820034\n",
      "episode_reward_min: -545228.857820034\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 1\n",
      "experiment_id: e72308bfadaf4ce5ad2335f68d334500\n",
      "hostname: ubuntu\n",
      "info:\n",
      "  grad_time_ms: .nan\n",
      "  learner: {}\n",
      "  max_exploration: 1.0\n",
      "  min_exploration: 1.0\n",
      "  num_steps_sampled: 1000\n",
      "  num_steps_trained: 0\n",
      "  num_target_updates: 166\n",
      "  opt_peak_throughput: 0.0\n",
      "  opt_samples: .nan\n",
      "  replay_time_ms: .nan\n",
      "  sample_time_ms: 1.337\n",
      "  update_time_ms: 0.001\n",
      "iterations_since_restore: 1\n",
      "node_ip: 192.168.0.11\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.666666666666668\n",
      "  ram_util_percent: 30.600000000000005\n",
      "pid: 4110\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.21058124500316577\n",
      "  mean_inference_ms: 0.7920177071006386\n",
      "  mean_processing_ms: 0.22159947024716006\n",
      "time_since_restore: 1.6397991180419922\n",
      "time_this_iter_s: 1.6397991180419922\n",
      "time_total_s: 1.6397991180419922\n",
      "timestamp: 1589239765\n",
      "timesteps_since_restore: 1000\n",
      "timesteps_this_iter: 1000\n",
      "timesteps_total: 1000\n",
      "training_iteration: 1\n",
      "\n",
      "Checkpoint saved at /home/christian/ray_results/DDPG_SupplyChainEnv_2020-05-11_18-29-220fb9_soc/checkpoint_1/checkpoint-1\n",
      "custom_metrics: {}\n",
      "date: 2020-05-11_18-29-47\n",
      "done: false\n",
      "episode_len_mean: 25.0\n",
      "episode_reward_max: -545228.857820034\n",
      "episode_reward_mean: -545228.857820034\n",
      "episode_reward_min: -545228.857820034\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 1\n",
      "experiment_id: e72308bfadaf4ce5ad2335f68d334500\n",
      "hostname: ubuntu\n",
      "info:\n",
      "  grad_time_ms: 25.939\n",
      "  learner:\n",
      "    default_policy:\n",
      "      max_q: -182127.15625\n",
      "      mean_q: -3700292.75\n",
      "      min_q: -5309109.5\n",
      "  max_exploration: 1.0\n",
      "  min_exploration: 1.0\n",
      "  num_steps_sampled: 2000\n",
      "  num_steps_trained: 128000\n",
      "  num_target_updates: 333\n",
      "  opt_peak_throughput: 9869.277\n",
      "  opt_samples: 256.0\n",
      "  replay_time_ms: 22.087\n",
      "  sample_time_ms: 3.003\n",
      "  update_time_ms: 0.004\n",
      "iterations_since_restore: 2\n",
      "node_ip: 192.168.0.11\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.854838709677416\n",
      "  ram_util_percent: 30.758064516129025\n",
      "pid: 4110\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.21058124500316577\n",
      "  mean_inference_ms: 0.7920177071006386\n",
      "  mean_processing_ms: 0.22159947024716006\n",
      "time_since_restore: 23.69760823249817\n",
      "time_this_iter_s: 22.057809114456177\n",
      "time_total_s: 23.69760823249817\n",
      "timestamp: 1589239787\n",
      "timesteps_since_restore: 2000\n",
      "timesteps_this_iter: 1000\n",
      "timesteps_total: 2000\n",
      "training_iteration: 2\n",
      "\n",
      "Checkpoint saved at /home/christian/ray_results/DDPG_SupplyChainEnv_2020-05-11_18-29-220fb9_soc/checkpoint_2/checkpoint-2\n",
      "custom_metrics: {}\n",
      "date: 2020-05-11_18-30-34\n",
      "done: false\n",
      "episode_len_mean: 25.0\n",
      "episode_reward_max: -545228.857820034\n",
      "episode_reward_mean: -545228.857820034\n",
      "episode_reward_min: -545228.857820034\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 1\n",
      "experiment_id: e72308bfadaf4ce5ad2335f68d334500\n",
      "hostname: ubuntu\n",
      "info:\n",
      "  grad_time_ms: 27.296\n",
      "  learner:\n",
      "    default_policy:\n",
      "      max_q: -19734.1953125\n",
      "      mean_q: -4370415.0\n",
      "      min_q: -5548110.0\n",
      "  max_exploration: 1.0\n",
      "  min_exploration: 1.0\n",
      "  num_steps_sampled: 3000\n",
      "  num_steps_trained: 384000\n",
      "  num_target_updates: 500\n",
      "  opt_peak_throughput: 9378.51\n",
      "  opt_samples: 256.0\n",
      "  replay_time_ms: 22.065\n",
      "  sample_time_ms: 3.683\n",
      "  update_time_ms: 0.003\n",
      "iterations_since_restore: 3\n",
      "node_ip: 192.168.0.11\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.245454545454546\n",
      "  ram_util_percent: 33.53181818181819\n",
      "pid: 4110\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.21058124500316577\n",
      "  mean_inference_ms: 0.7920177071006386\n",
      "  mean_processing_ms: 0.22159947024716006\n",
      "time_since_restore: 70.0937716960907\n",
      "time_this_iter_s: 46.39616346359253\n",
      "time_total_s: 70.0937716960907\n",
      "timestamp: 1589239834\n",
      "timesteps_since_restore: 3000\n",
      "timesteps_this_iter: 1000\n",
      "timesteps_total: 3000\n",
      "training_iteration: 3\n",
      "\n",
      "Checkpoint saved at /home/christian/ray_results/DDPG_SupplyChainEnv_2020-05-11_18-29-220fb9_soc/checkpoint_3/checkpoint-3\n",
      "custom_metrics: {}\n",
      "date: 2020-05-11_18-31-22\n",
      "done: false\n",
      "episode_len_mean: 25.0\n",
      "episode_reward_max: -545228.857820034\n",
      "episode_reward_mean: -545228.857820034\n",
      "episode_reward_min: -545228.857820034\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 1\n",
      "experiment_id: e72308bfadaf4ce5ad2335f68d334500\n",
      "hostname: ubuntu\n",
      "info:\n",
      "  grad_time_ms: 26.311\n",
      "  learner:\n",
      "    default_policy:\n",
      "      max_q: -4236.54150390625\n",
      "      mean_q: -6335452.0\n",
      "      min_q: -8447083.0\n",
      "  max_exploration: 1.0\n",
      "  min_exploration: 1.0\n",
      "  num_steps_sampled: 4000\n",
      "  num_steps_trained: 640000\n",
      "  num_target_updates: 666\n",
      "  opt_peak_throughput: 9729.633\n",
      "  opt_samples: 256.0\n",
      "  replay_time_ms: 18.841\n",
      "  sample_time_ms: 2.778\n",
      "  update_time_ms: 0.003\n",
      "iterations_since_restore: 4\n",
      "node_ip: 192.168.0.11\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 23.014492753623188\n",
      "  ram_util_percent: 35.052173913043475\n",
      "pid: 4110\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.21058124500316577\n",
      "  mean_inference_ms: 0.7920177071006386\n",
      "  mean_processing_ms: 0.22159947024716006\n",
      "time_since_restore: 118.16964912414551\n",
      "time_this_iter_s: 48.07587742805481\n",
      "time_total_s: 118.16964912414551\n",
      "timestamp: 1589239882\n",
      "timesteps_since_restore: 4000\n",
      "timesteps_this_iter: 1000\n",
      "timesteps_total: 4000\n",
      "training_iteration: 4\n",
      "\n",
      "Checkpoint saved at /home/christian/ray_results/DDPG_SupplyChainEnv_2020-05-11_18-29-220fb9_soc/checkpoint_4/checkpoint-4\n",
      "custom_metrics: {}\n",
      "date: 2020-05-11_18-32-08\n",
      "done: false\n",
      "episode_len_mean: 25.0\n",
      "episode_reward_max: -545228.857820034\n",
      "episode_reward_mean: -545228.857820034\n",
      "episode_reward_min: -545228.857820034\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 1\n",
      "experiment_id: e72308bfadaf4ce5ad2335f68d334500\n",
      "hostname: ubuntu\n",
      "info:\n",
      "  grad_time_ms: 24.78\n",
      "  learner:\n",
      "    default_policy:\n",
      "      max_q: -83498.7109375\n",
      "      mean_q: -7090881.0\n",
      "      min_q: -12562013.0\n",
      "  max_exploration: 1.0\n",
      "  min_exploration: 1.0\n",
      "  num_steps_sampled: 5000\n",
      "  num_steps_trained: 896000\n",
      "  num_target_updates: 833\n",
      "  opt_peak_throughput: 10331.066\n",
      "  opt_samples: 256.0\n",
      "  replay_time_ms: 23.052\n",
      "  sample_time_ms: 2.378\n",
      "  update_time_ms: 0.004\n",
      "iterations_since_restore: 5\n",
      "node_ip: 192.168.0.11\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 20.387692307692305\n",
      "  ram_util_percent: 35.20153846153846\n",
      "pid: 4110\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.21058124500316577\n",
      "  mean_inference_ms: 0.7920177071006386\n",
      "  mean_processing_ms: 0.22159947024716006\n",
      "time_since_restore: 164.16492414474487\n",
      "time_this_iter_s: 45.995275020599365\n",
      "time_total_s: 164.16492414474487\n",
      "timestamp: 1589239928\n",
      "timesteps_since_restore: 5000\n",
      "timesteps_this_iter: 1000\n",
      "timesteps_total: 5000\n",
      "training_iteration: 5\n",
      "\n",
      "Checkpoint saved at /home/christian/ray_results/DDPG_SupplyChainEnv_2020-05-11_18-29-220fb9_soc/checkpoint_5/checkpoint-5\n",
      "custom_metrics: {}\n",
      "date: 2020-05-11_18-32-53\n",
      "done: false\n",
      "episode_len_mean: 25.0\n",
      "episode_reward_max: -545228.857820034\n",
      "episode_reward_mean: -545228.857820034\n",
      "episode_reward_min: -545228.857820034\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 1\n",
      "experiment_id: e72308bfadaf4ce5ad2335f68d334500\n",
      "hostname: ubuntu\n",
      "info:\n",
      "  grad_time_ms: 26.934\n",
      "  learner:\n",
      "    default_policy:\n",
      "      max_q: -225877.859375\n",
      "      mean_q: -12740078.0\n",
      "      min_q: -18751272.0\n",
      "  max_exploration: 1.0\n",
      "  min_exploration: 1.0\n",
      "  num_steps_sampled: 6000\n",
      "  num_steps_trained: 1152000\n",
      "  num_target_updates: 1000\n",
      "  opt_peak_throughput: 9504.806\n",
      "  opt_samples: 256.0\n",
      "  replay_time_ms: 28.341\n",
      "  sample_time_ms: 4.071\n",
      "  update_time_ms: 0.003\n",
      "iterations_since_restore: 6\n",
      "node_ip: 192.168.0.11\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 17.321538461538463\n",
      "  ram_util_percent: 35.449230769230766\n",
      "pid: 4110\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.21058124500316577\n",
      "  mean_inference_ms: 0.7920177071006386\n",
      "  mean_processing_ms: 0.22159947024716006\n",
      "time_since_restore: 209.74008584022522\n",
      "time_this_iter_s: 45.57516169548035\n",
      "time_total_s: 209.74008584022522\n",
      "timestamp: 1589239973\n",
      "timesteps_since_restore: 6000\n",
      "timesteps_this_iter: 1000\n",
      "timesteps_total: 6000\n",
      "training_iteration: 6\n",
      "\n",
      "Checkpoint saved at /home/christian/ray_results/DDPG_SupplyChainEnv_2020-05-11_18-29-220fb9_soc/checkpoint_6/checkpoint-6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_metrics: {}\n",
      "date: 2020-05-11_18-33-41\n",
      "done: false\n",
      "episode_len_mean: 25.0\n",
      "episode_reward_max: -545228.857820034\n",
      "episode_reward_mean: -545228.857820034\n",
      "episode_reward_min: -545228.857820034\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 1\n",
      "experiment_id: e72308bfadaf4ce5ad2335f68d334500\n",
      "hostname: ubuntu\n",
      "info:\n",
      "  grad_time_ms: 21.717\n",
      "  learner:\n",
      "    default_policy:\n",
      "      max_q: -5558.4970703125\n",
      "      mean_q: -14923014.0\n",
      "      min_q: -27734554.0\n",
      "  max_exploration: 1.0\n",
      "  min_exploration: 1.0\n",
      "  num_steps_sampled: 7000\n",
      "  num_steps_trained: 1408000\n",
      "  num_target_updates: 1166\n",
      "  opt_peak_throughput: 11787.819\n",
      "  opt_samples: 256.0\n",
      "  replay_time_ms: 18.314\n",
      "  sample_time_ms: 2.567\n",
      "  update_time_ms: 0.002\n",
      "iterations_since_restore: 7\n",
      "node_ip: 192.168.0.11\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 20.441176470588236\n",
      "  ram_util_percent: 35.76323529411763\n",
      "pid: 4110\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.21058124500316577\n",
      "  mean_inference_ms: 0.7920177071006386\n",
      "  mean_processing_ms: 0.22159947024716006\n",
      "time_since_restore: 257.66602993011475\n",
      "time_this_iter_s: 47.925944089889526\n",
      "time_total_s: 257.66602993011475\n",
      "timestamp: 1589240021\n",
      "timesteps_since_restore: 7000\n",
      "timesteps_this_iter: 1000\n",
      "timesteps_total: 7000\n",
      "training_iteration: 7\n",
      "\n",
      "Checkpoint saved at /home/christian/ray_results/DDPG_SupplyChainEnv_2020-05-11_18-29-220fb9_soc/checkpoint_7/checkpoint-7\n",
      "custom_metrics: {}\n",
      "date: 2020-05-11_18-34-24\n",
      "done: false\n",
      "episode_len_mean: 25.0\n",
      "episode_reward_max: -545228.857820034\n",
      "episode_reward_mean: -545228.857820034\n",
      "episode_reward_min: -545228.857820034\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 1\n",
      "experiment_id: e72308bfadaf4ce5ad2335f68d334500\n",
      "hostname: ubuntu\n",
      "info:\n",
      "  grad_time_ms: 21.164\n",
      "  learner:\n",
      "    default_policy:\n",
      "      max_q: -2309.248046875\n",
      "      mean_q: -19729222.0\n",
      "      min_q: -37332440.0\n",
      "  max_exploration: 1.0\n",
      "  min_exploration: 1.0\n",
      "  num_steps_sampled: 8000\n",
      "  num_steps_trained: 1664000\n",
      "  num_target_updates: 1333\n",
      "  opt_peak_throughput: 12096.169\n",
      "  opt_samples: 256.0\n",
      "  replay_time_ms: 18.332\n",
      "  sample_time_ms: 2.156\n",
      "  update_time_ms: 0.002\n",
      "iterations_since_restore: 8\n",
      "node_ip: 192.168.0.11\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.400000000000002\n",
      "  ram_util_percent: 36.06721311475409\n",
      "pid: 4110\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.21058124500316577\n",
      "  mean_inference_ms: 0.7920177071006386\n",
      "  mean_processing_ms: 0.22159947024716006\n",
      "time_since_restore: 300.7476632595062\n",
      "time_this_iter_s: 43.08163332939148\n",
      "time_total_s: 300.7476632595062\n",
      "timestamp: 1589240064\n",
      "timesteps_since_restore: 8000\n",
      "timesteps_this_iter: 1000\n",
      "timesteps_total: 8000\n",
      "training_iteration: 8\n",
      "\n",
      "Checkpoint saved at /home/christian/ray_results/DDPG_SupplyChainEnv_2020-05-11_18-29-220fb9_soc/checkpoint_8/checkpoint-8\n",
      "custom_metrics: {}\n",
      "date: 2020-05-11_18-35-09\n",
      "done: false\n",
      "episode_len_mean: 25.0\n",
      "episode_reward_max: -545228.857820034\n",
      "episode_reward_mean: -545228.857820034\n",
      "episode_reward_min: -545228.857820034\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 1\n",
      "experiment_id: e72308bfadaf4ce5ad2335f68d334500\n",
      "hostname: ubuntu\n",
      "info:\n",
      "  grad_time_ms: 21.627\n",
      "  learner:\n",
      "    default_policy:\n",
      "      max_q: -608465.25\n",
      "      mean_q: -30130058.0\n",
      "      min_q: -49175512.0\n",
      "  max_exploration: 1.0\n",
      "  min_exploration: 1.0\n",
      "  num_steps_sampled: 9000\n",
      "  num_steps_trained: 1920000\n",
      "  num_target_updates: 1500\n",
      "  opt_peak_throughput: 11837.305\n",
      "  opt_samples: 256.0\n",
      "  replay_time_ms: 18.809\n",
      "  sample_time_ms: 2.307\n",
      "  update_time_ms: 0.002\n",
      "iterations_since_restore: 9\n",
      "node_ip: 192.168.0.11\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.575000000000003\n",
      "  ram_util_percent: 36.1703125\n",
      "pid: 4110\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.21058124500316577\n",
      "  mean_inference_ms: 0.7920177071006386\n",
      "  mean_processing_ms: 0.22159947024716006\n",
      "time_since_restore: 345.6441402435303\n",
      "time_this_iter_s: 44.89647698402405\n",
      "time_total_s: 345.6441402435303\n",
      "timestamp: 1589240109\n",
      "timesteps_since_restore: 9000\n",
      "timesteps_this_iter: 1000\n",
      "timesteps_total: 9000\n",
      "training_iteration: 9\n",
      "\n",
      "Checkpoint saved at /home/christian/ray_results/DDPG_SupplyChainEnv_2020-05-11_18-29-220fb9_soc/checkpoint_9/checkpoint-9\n",
      "custom_metrics: {}\n",
      "date: 2020-05-11_18-35-53\n",
      "done: false\n",
      "episode_len_mean: 25.0\n",
      "episode_reward_max: -545228.857820034\n",
      "episode_reward_mean: -545228.857820034\n",
      "episode_reward_min: -545228.857820034\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 1\n",
      "experiment_id: e72308bfadaf4ce5ad2335f68d334500\n",
      "hostname: ubuntu\n",
      "info:\n",
      "  grad_time_ms: 23.32\n",
      "  learner:\n",
      "    default_policy:\n",
      "      max_q: -2479507.5\n",
      "      mean_q: -41917356.0\n",
      "      min_q: -64040768.0\n",
      "  max_exploration: 1.0\n",
      "  min_exploration: 1.0\n",
      "  num_steps_sampled: 10000\n",
      "  num_steps_trained: 2176000\n",
      "  num_target_updates: 1666\n",
      "  opt_peak_throughput: 10977.552\n",
      "  opt_samples: 256.0\n",
      "  replay_time_ms: 18.141\n",
      "  sample_time_ms: 2.068\n",
      "  update_time_ms: 0.003\n",
      "iterations_since_restore: 10\n",
      "node_ip: 192.168.0.11\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.352459016393444\n",
      "  ram_util_percent: 36.2\n",
      "pid: 4110\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.21058124500316577\n",
      "  mean_inference_ms: 0.7920177071006386\n",
      "  mean_processing_ms: 0.22159947024716006\n",
      "time_since_restore: 389.1610789299011\n",
      "time_this_iter_s: 43.51693868637085\n",
      "time_total_s: 389.1610789299011\n",
      "timestamp: 1589240153\n",
      "timesteps_since_restore: 10000\n",
      "timesteps_this_iter: 1000\n",
      "timesteps_total: 10000\n",
      "training_iteration: 10\n",
      "\n",
      "Checkpoint saved at /home/christian/ray_results/DDPG_SupplyChainEnv_2020-05-11_18-29-220fb9_soc/checkpoint_10/checkpoint-10\n"
     ]
    }
   ],
   "source": [
    "trainer, result = train_ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_config(default_config, config_dict=None):\n",
    "    config = deepcopy(default_config)\n",
    "    if type(config_dict) == dict:\n",
    "        for k in config.keys():\n",
    "            if k in config_dict.keys():\n",
    "                if type(config[k]) == dict:\n",
    "                    for m in config[k].keys():\n",
    "                        if m in config_dict.keys():\n",
    "                            config[k][m] = config_dict[m]\n",
    "                else:\n",
    "                    config[k] = config_dict[k]\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "    return config\n",
    "\n",
    "def train_agent(algo='a3c', iters=10, config_dict={}):\n",
    "    if hasattr(agents, algo):\n",
    "        agent = getattr(agents, algo)\n",
    "        config = set_config(agent.DEFAULT_CONFIG, config_dict)\n",
    "        trainer = getattr(agent, algo.upper() + 'Trainer')(config, env=SupplyChainEnv)\n",
    "    else:\n",
    "        raise AttributeError('No attribute {}'.format(algo))\n",
    "    \n",
    "    results = []\n",
    "    for n in range(iters):\n",
    "        result = trainer.train()\n",
    "#         print(pretty_print(result))\n",
    "        results.append(result)\n",
    "        if (n + 1) % 10 == 0:\n",
    "            print(\"Iter:\\t{}\\tMean Rewards:\\t{:.1f}\".format(n+1, result['episode_reward_mean']))\n",
    "    \n",
    "    return trainer, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-12 09:29:30,141\tWARNING util.py:45 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:\t10\tMean Rewards:\t-626198.1\n",
      "\u001b[2m\u001b[36m(pid=19380)\u001b[0m Exception in thread Thread-1:\n",
      "\u001b[2m\u001b[36m(pid=19380)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=19380)\u001b[0m   File \"/home/christian/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(pid=19380)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(pid=19380)\u001b[0m   File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py\", line 173, in run\n",
      "\u001b[2m\u001b[36m(pid=19380)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(pid=19380)\u001b[0m   File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py\", line 170, in run\n",
      "\u001b[2m\u001b[36m(pid=19380)\u001b[0m     self._run()\n",
      "\u001b[2m\u001b[36m(pid=19380)\u001b[0m   File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py\", line 193, in _run\n",
      "\u001b[2m\u001b[36m(pid=19380)\u001b[0m     item = next(rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=19380)\u001b[0m   File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py\", line 340, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=19380)\u001b[0m     base_env.send_actions(actions_to_send)\n",
      "\u001b[2m\u001b[36m(pid=19380)\u001b[0m   File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/env/base_env.py\", line 332, in send_actions\n",
      "\u001b[2m\u001b[36m(pid=19380)\u001b[0m     self.vector_env.vector_step(action_vector)\n",
      "\u001b[2m\u001b[36m(pid=19380)\u001b[0m   File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/env/vector_env.py\", line 110, in vector_step\n",
      "\u001b[2m\u001b[36m(pid=19380)\u001b[0m     obs, r, done, info = self.envs[i].step(actions[i])\n",
      "\u001b[2m\u001b[36m(pid=19380)\u001b[0m   File \"<ipython-input-20-94a94a9d4018>\", line 103, in step\n",
      "\u001b[2m\u001b[36m(pid=19380)\u001b[0m ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\u001b[2m\u001b[36m(pid=19380)\u001b[0m \n"
     ]
    },
    {
     "ename": "RayTaskError(RuntimeError)",
     "evalue": "\u001b[36mray::RolloutWorker.sample()\u001b[39m (pid=19380, ip=192.168.0.11)\n  File \"python/ray/_raylet.pyx\", line 629, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 630, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 519, in ray._raylet.deserialize_args\nray.exceptions.RayTaskError: \u001b[36mray::RolloutWorker.sample()\u001b[39m (pid=19380, ip=192.168.0.11)\n  File \"python/ray/_raylet.pyx\", line 636, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 619, in ray._raylet.execute_task.function_executor\n  File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 471, in sample\n    batches = [self.input_reader.next()]\n  File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py\", line 56, in next\n    batches = [self.get_data()]\n  File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py\", line 201, in get_data\n    raise RuntimeError(\"Sampling thread has died\")\nRuntimeError: Sampling thread has died",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(RuntimeError)\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-0c383cb032cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                'buffer_size': 10000}\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a3c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-773d38487a28>\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m(algo, iters, config_dict)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;31m#         print(pretty_print(result))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    442\u001b[0m                         \u001b[0;34m\"continue training without the failed worker, set \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                         \"`'ignore_worker_failures': True`.\")\n\u001b[0;32m--> 444\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# allow logs messages to propagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMAX_WORKER_FAILURE_RETRIES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRayError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ignore_worker_failures\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ray/tune/trainable.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \"\"\"\n\u001b[1;32m    175\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_train() needs to return a dict.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mafter_optimizer_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mafter_optimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ray/rllib/optimizers/async_gradients_optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mready_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray_get_and_free\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpending_gradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearner_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_learner_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ray/rllib/utils/memory.py\u001b[0m in \u001b[0;36mray_get_and_free\u001b[0;34m(object_ids)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_to_free\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mobject_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mobject_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_ids, timeout)\u001b[0m\n\u001b[1;32m   1455\u001b[0m                     \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore_worker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_object_store_memory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRayTaskError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_instanceof_cause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1458\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRayTaskError(RuntimeError)\u001b[0m: \u001b[36mray::RolloutWorker.sample()\u001b[39m (pid=19380, ip=192.168.0.11)\n  File \"python/ray/_raylet.pyx\", line 629, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 630, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 519, in ray._raylet.deserialize_args\nray.exceptions.RayTaskError: \u001b[36mray::RolloutWorker.sample()\u001b[39m (pid=19380, ip=192.168.0.11)\n  File \"python/ray/_raylet.pyx\", line 636, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 619, in ray._raylet.execute_task.function_executor\n  File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 471, in sample\n    batches = [self.input_reader.next()]\n  File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py\", line 56, in next\n    batches = [self.get_data()]\n  File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py\", line 201, in get_data\n    raise RuntimeError(\"Sampling thread has died\")\nRuntimeError: Sampling thread has died"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19379)\u001b[0m Exception in thread Thread-1:\n",
      "\u001b[2m\u001b[36m(pid=19379)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=19379)\u001b[0m   File \"/home/christian/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(pid=19379)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(pid=19379)\u001b[0m   File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py\", line 173, in run\n",
      "\u001b[2m\u001b[36m(pid=19379)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(pid=19379)\u001b[0m   File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py\", line 170, in run\n",
      "\u001b[2m\u001b[36m(pid=19379)\u001b[0m     self._run()\n",
      "\u001b[2m\u001b[36m(pid=19379)\u001b[0m   File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py\", line 193, in _run\n",
      "\u001b[2m\u001b[36m(pid=19379)\u001b[0m     item = next(rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=19379)\u001b[0m   File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py\", line 340, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=19379)\u001b[0m     base_env.send_actions(actions_to_send)\n",
      "\u001b[2m\u001b[36m(pid=19379)\u001b[0m   File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/env/base_env.py\", line 332, in send_actions\n",
      "\u001b[2m\u001b[36m(pid=19379)\u001b[0m     self.vector_env.vector_step(action_vector)\n",
      "\u001b[2m\u001b[36m(pid=19379)\u001b[0m   File \"/home/christian/anaconda3/lib/python3.6/site-packages/ray/rllib/env/vector_env.py\", line 110, in vector_step\n",
      "\u001b[2m\u001b[36m(pid=19379)\u001b[0m     obs, r, done, info = self.envs[i].step(actions[i])\n",
      "\u001b[2m\u001b[36m(pid=19379)\u001b[0m   File \"<ipython-input-20-94a94a9d4018>\", line 103, in step\n",
      "\u001b[2m\u001b[36m(pid=19379)\u001b[0m ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\u001b[2m\u001b[36m(pid=19379)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-12 11:29:32,736\tERROR worker.py:1056 -- listen_error_messages_raylet: Connection closed by server.\n",
      "2020-05-12 11:29:32,739\tERROR import_thread.py:97 -- ImportThread: Connection closed by server.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m *** Aborted at 1589300972 (unix time) try \"date -d @1589300972\" if you are using GNU date ***\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m PC: @                0x0 (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m *** SIGTERM (@0x3e8000056d8) received by PID 19364 (TID 0x7faf07d7b740) from PID 22232; stack trace: ***\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7faf072f0390 (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7faf06bfe9f3 epoll_wait\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @           0x418e1c boost::asio::detail::epoll_reactor::run()\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @           0x4194b9 boost::asio::detail::scheduler::run()\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @           0x409c5f main\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7faf06b17830 __libc_start_main\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @           0x40ed81 (unknown)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-12 11:29:32,748\tERROR worker.py:956 -- print_logs: Connection closed by server.\n"
     ]
    }
   ],
   "source": [
    "config_dict = {'actor_hiddens': [128, 128],\n",
    "               'critic_hiddens': [128, 128],\n",
    "               'timesteps_per_iteration': 1000,\n",
    "               'buffer_size': 10000}\n",
    "\n",
    "agent, results = train_agent('a3c', 200, config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rewards = [i['episode_reward_mean'] for i in results]\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(mean_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-608386.6926 -923634.0\n"
     ]
    }
   ],
   "source": [
    "env = SupplyChainEnv({})\n",
    "rewards = []\n",
    "for i in range(10000):\n",
    "    env.reset()\n",
    "    R = 0\n",
    "    done = False\n",
    "    while done == False:\n",
    "        s, r, done, _ = env.step(env.action_space.sample())\n",
    "        R += r\n",
    "        if done:\n",
    "            rewards.append(R)\n",
    "            \n",
    "print(np.mean(rewards), np.min(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-306666.0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
