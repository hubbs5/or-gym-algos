{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametric Actions\n",
    "\n",
    "One of the ways to enforce constraints on the model to restrict the actions available to an agent in certain states. This ought to improve learning and speed results because some areas of the state space will be off limits. \n",
    "\n",
    "Take for example an online knpasack problem. The agent is given an item at every time step and must decide whether or not to accept the item and pack it into the sack, reject it and get a new one, or close up the sack and end the episode. We could simply provide a large negative reward for the agent if it were to accept the item and go over the weight limit, but it would be more efficacious to block the agent from accepting the item in these situations so that it now has two options: end the episode or reject the item.\n",
    "\n",
    "Here, I'll implement a simple knapsack environment to limit the algorithm from selecting items that cause it to exceed its limit. There will be three actions available to the algorithm.\n",
    "\n",
    "- 0: end episode\n",
    "- 1: accept item\n",
    "- 2: reject item\n",
    "\n",
    "If 0 is selected, the episode ends and the agent collects no additional reward. If 1 is selected, the agent packs that item and collects the reward. If 2 is selected, the agent rejects the item and moves to the next. \n",
    "\n",
    "If the parametric action selection works properly, the agent should never exceed the capacity of the knapsack and receive a large, negative reward.\n",
    "\n",
    "For this, I'm following the example laid out in the [Ray code for the parametric cartpole](https://github.com/ray-project/ray/blob/master/rllib/examples/parametric_action_cartpole.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ray\n",
    "from ray import tune\n",
    "import gym\n",
    "from gym import spaces\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.tf.fcnet_v2 import FullyConnectedNetwork\n",
    "from ray.rllib.utils import try_import_tf\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from or_gym.utils.env_config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Environment\n",
    "\n",
    "We need to set up the environment to interact with the Ray framework properly so that the forbidden actions are masked given the goals outlined above. In this case, it's rather easy, we'll simply look to see if our next accepted item plus our current weight is greater than our weight capacity.\n",
    "\n",
    "Referring to the Ray code (see lines 69-73 in the above link), we need to place our actions into a dictionary using the `spaces.Dict` function. This dictionary needs to include the normal state from our environment as well as an action mask and the available actions we can choose from. For our state, we'll only have three outputs, the current weight of the knapsack, the value of the next item, and the weight of the next item.\n",
    "\n",
    "As discussed above, we also have three actions to choose from, so we'll need a corresponding list of three outputs for the mask and available actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParametricKnapsack(gym.Env):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.step_limit = 10\n",
    "        self.item_values = np.random.randint(0, 10, self.step_limit)\n",
    "        self.item_weights = np.random.randint(1, 5, self.step_limit)\n",
    "        self.weight_capacity = 20\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.mask = True\n",
    "        assign_env_config(self, kwargs)\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"action_mask\": spaces.Box(0, 1, shape=(3,)),\n",
    "            \"avail_actions\": spaces.Box(0, 1, shape=(3,)),\n",
    "            \"state\": spaces.Box(0, self.weight_capacity, shape=(3,))\n",
    "        })\n",
    "\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_weight, self.current_step = 0, 0\n",
    "        self.item_values = np.random.randint(0, 10, self.step_limit)\n",
    "        self.item_weights = np.random.randint(1, 5, self.step_limit)\n",
    "        self.state = {\n",
    "            \"action_mask\": np.ones(3),\n",
    "            \"avail_actions\": np.ones(3),\n",
    "            \"state\": np.array(\n",
    "                [self.current_weight, \n",
    "                 self.item_values[self.current_step], \n",
    "                 self.item_weights[self.current_step]])}\n",
    "        self.update_state()\n",
    "\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.current_weight = self.state[\"state\"][0]\n",
    "        item_value = self.state[\"state\"][1]\n",
    "        item_weight = self.state[\"state\"][2]\n",
    "        done = False\n",
    "        if action == 0:\n",
    "            # End episode\n",
    "            done = True\n",
    "            reward = 0\n",
    "        elif action == 1:\n",
    "            # Accept item\n",
    "            if self.current_weight + item_weight <= self.weight_capacity:\n",
    "                self.current_weight += item_weight\n",
    "                reward = item_value\n",
    "                # End if capacity is met\n",
    "                if self.current_weight == self.weight_capacity:\n",
    "                    done = True\n",
    "            else: # Overweight\n",
    "                reward = -100\n",
    "                done = True\n",
    "        elif action == 2:\n",
    "            # Reject item\n",
    "            reward = 0\n",
    "        \n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.step_limit:\n",
    "            done = True\n",
    "        self.update_state()\n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "    def update_state(self):\n",
    "        # Make action selection impossible if the knapsack would go over weight\n",
    "        step = self.current_step if self.current_step < self.step_limit else self.step_limit-1\n",
    "        knapsack = np.array([self.current_weight, \n",
    "                self.item_values[step], \n",
    "                self.item_weights[step]])\n",
    "        action_mask = np.ones(3)\n",
    "        if self.mask:\n",
    "            if self.current_weight + knapsack[-1] > self.weight_capacity:\n",
    "                action_mask = np.array([1, 0, 1])\n",
    "            \n",
    "        self.state = {\n",
    "                \"action_mask\": action_mask,\n",
    "                \"avail_actions\": np.ones(3),\n",
    "                \"state\": knapsack\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = try_import_tf()\n",
    "\n",
    "class KPParametricActionsModel(TFModelV2):\n",
    "    \n",
    "    def __init__(self, obs_space, action_space, num_outputs,\n",
    "        model_config, name, true_obs_shape=(3,), action_embed_size=3,\n",
    "        *args, **kwargs):\n",
    "        super(KPParametricActionsModel, self).__init__(obs_space,\n",
    "            action_space, num_outputs, model_config, name, *args, **kwargs)\n",
    "        self.action_embed_model = FullyConnectedNetwork(\n",
    "            spaces.Box(-1, 1, shape=true_obs_shape), action_space, action_embed_size,\n",
    "            model_config, name + \"_action_embedding\")\n",
    "        self.register_variables(self.action_embed_model.variables())\n",
    "        \n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        avail_actions = input_dict[\"obs\"][\"avail_actions\"]\n",
    "        action_mask = input_dict[\"obs\"][\"action_mask\"]\n",
    "        action_embedding, _ = self.action_embed_model({\n",
    "            \"obs\": input_dict[\"obs\"][\"state\"]\n",
    "        })\n",
    "        intent_vector = tf.expand_dims(action_embedding, 1)\n",
    "        action_logits = tf.reduce_sum(avail_actions * intent_vector, axis=1)\n",
    "        inf_mask = tf.maximum(tf.log(action_mask), tf.float32.min)\n",
    "        return action_logits + inf_mask, state\n",
    "    \n",
    "    def value_function(self):\n",
    "        return self.action_embed_model.value_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-23 17:46:13,907\tWARNING services.py:597 -- setpgrp failed, processes may not be cleaned up properly: [Errno 1] Operation not permitted.\n",
      "2020-04-23 17:46:13,909\tINFO resource_spec.py:216 -- Starting Ray with 3.47 GiB memory available for workers and up to 1.75 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-04-23 17:46:14,475\tINFO ray_trial_executor.py:121 -- Trial PPO_ParaKnapsack-v0_3c894ede: Setting up new remote runner.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=4758)\u001b[0m 2020-04-23 17:46:17,898\tINFO trainer.py:371 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=4758)\u001b[0m 2020-04-23 17:46:17,900\tINFO trainer.py:512 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=4758)\u001b[0m 2020-04-23 17:46:20,597\tWARNING util.py:45 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=4758)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:151: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(pid=4758)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-23 17:47:06,374\tINFO tune.py:334 -- Returning an analysis object by default. You can call `analysis.trials` to retrieve a list of trials. This message will be removed in future versions of Tune.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>timesteps_this_iter</th>\n",
       "      <th>done</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>...</th>\n",
       "      <th>info/learner/default_policy/policy_loss</th>\n",
       "      <th>info/learner/default_policy/vf_loss</th>\n",
       "      <th>info/learner/default_policy/vf_explained_var</th>\n",
       "      <th>info/learner/default_policy/kl</th>\n",
       "      <th>info/learner/default_policy/entropy</th>\n",
       "      <th>info/learner/default_policy/entropy_coeff</th>\n",
       "      <th>config/env</th>\n",
       "      <th>config/env_config</th>\n",
       "      <th>config/model</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.453585</td>\n",
       "      <td>3.015094</td>\n",
       "      <td>1325</td>\n",
       "      <td>4000</td>\n",
       "      <td>True</td>\n",
       "      <td>40000</td>\n",
       "      <td>13566</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.093364</td>\n",
       "      <td>36.640034</td>\n",
       "      <td>0.028345</td>\n",
       "      <td>0.014002</td>\n",
       "      <td>1.084052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ParaKnapsack-v0</td>\n",
       "      <td>{'mask': True}</td>\n",
       "      <td>{'custom_model': 'kp_param_model'}</td>\n",
       "      <td>/home/christian/ray_results/PPO/PPO_ParaKnapsa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "0                39.0                 0.0             4.453585   \n",
       "\n",
       "   episode_len_mean  episodes_this_iter  timesteps_this_iter  done  \\\n",
       "0          3.015094                1325                 4000  True   \n",
       "\n",
       "   timesteps_total  episodes_total  training_iteration  ...  \\\n",
       "0            40000           13566                  10  ...   \n",
       "\n",
       "  info/learner/default_policy/policy_loss info/learner/default_policy/vf_loss  \\\n",
       "0                               -0.093364                           36.640034   \n",
       "\n",
       "   info/learner/default_policy/vf_explained_var  \\\n",
       "0                                      0.028345   \n",
       "\n",
       "   info/learner/default_policy/kl  info/learner/default_policy/entropy  \\\n",
       "0                        0.014002                             1.084052   \n",
       "\n",
       "   info/learner/default_policy/entropy_coeff       config/env  \\\n",
       "0                                        0.0  ParaKnapsack-v0   \n",
       "\n",
       "  config/env_config                        config/model  \\\n",
       "0    {'mask': True}  {'custom_model': 'kp_param_model'}   \n",
       "\n",
       "                                              logdir  \n",
       "0  /home/christian/ray_results/PPO/PPO_ParaKnapsa...  \n",
       "\n",
       "[1 rows x 48 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_env(config_env):\n",
    "    return ParametricKnapsack()\n",
    "\n",
    "ModelCatalog.register_custom_model(\"kp_param_model\", KPParametricActionsModel)\n",
    "tune.register_env(\"ParaKnapsack-v0\", lambda config: create_env(config))\n",
    "\n",
    "# ray.init(ignore_reinit_error=True)\n",
    "\n",
    "results = tune.run(\n",
    "        \"PPO\",\n",
    "        stop={\"training_iteration\": 10},\n",
    "        config={\n",
    "            \"env\": \"ParaKnapsack-v0\",\n",
    "            \"env_config\": {\n",
    "                \"mask\": True\n",
    "            },\n",
    "            \"model\": {\n",
    "                \"custom_model\": \"kp_param_model\"\n",
    "            },\n",
    "        },\n",
    "        verbose=0,\n",
    "        reuse_actors=True)\n",
    "\n",
    "df = results.dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-23 17:47:06,461\tERROR worker.py:679 -- Calling ray.init() again after it has already been called.\n",
      "2020-04-23 17:47:06,480\tINFO ray_trial_executor.py:121 -- Trial PPO_ParaKnapsack-v0_5b8bdcfc: Setting up new remote runner.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=4761)\u001b[0m 2020-04-23 17:47:08,956\tINFO trainer.py:371 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=4761)\u001b[0m 2020-04-23 17:47:08,958\tINFO trainer.py:512 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=4761)\u001b[0m 2020-04-23 17:47:12,178\tWARNING util.py:45 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=4761)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:151: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(pid=4761)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-23 17:48:04,994\tINFO tune.py:334 -- Returning an analysis object by default. You can call `analysis.trials` to retrieve a list of trials. This message will be removed in future versions of Tune.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>timesteps_this_iter</th>\n",
       "      <th>done</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>...</th>\n",
       "      <th>info/learner/default_policy/policy_loss</th>\n",
       "      <th>info/learner/default_policy/vf_loss</th>\n",
       "      <th>info/learner/default_policy/vf_explained_var</th>\n",
       "      <th>info/learner/default_policy/kl</th>\n",
       "      <th>info/learner/default_policy/entropy</th>\n",
       "      <th>info/learner/default_policy/entropy_coeff</th>\n",
       "      <th>config/env</th>\n",
       "      <th>config/env_config</th>\n",
       "      <th>config/model</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.435268</td>\n",
       "      <td>2.97619</td>\n",
       "      <td>1344</td>\n",
       "      <td>4000</td>\n",
       "      <td>True</td>\n",
       "      <td>40000</td>\n",
       "      <td>13505</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.098762</td>\n",
       "      <td>36.736065</td>\n",
       "      <td>0.019587</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>1.083382</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ParaKnapsack-v0</td>\n",
       "      <td>{'mask': False}</td>\n",
       "      <td>{'custom_model': 'kp_param_model'}</td>\n",
       "      <td>/home/christian/ray_results/PPO/PPO_ParaKnapsa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "0                43.0                 0.0             4.435268   \n",
       "\n",
       "   episode_len_mean  episodes_this_iter  timesteps_this_iter  done  \\\n",
       "0           2.97619                1344                 4000  True   \n",
       "\n",
       "   timesteps_total  episodes_total  training_iteration  ...  \\\n",
       "0            40000           13505                  10  ...   \n",
       "\n",
       "  info/learner/default_policy/policy_loss info/learner/default_policy/vf_loss  \\\n",
       "0                               -0.098762                           36.736065   \n",
       "\n",
       "   info/learner/default_policy/vf_explained_var  \\\n",
       "0                                      0.019587   \n",
       "\n",
       "   info/learner/default_policy/kl  info/learner/default_policy/entropy  \\\n",
       "0                          0.0146                             1.083382   \n",
       "\n",
       "   info/learner/default_policy/entropy_coeff       config/env  \\\n",
       "0                                        0.0  ParaKnapsack-v0   \n",
       "\n",
       "  config/env_config                        config/model  \\\n",
       "0   {'mask': False}  {'custom_model': 'kp_param_model'}   \n",
       "\n",
       "                                              logdir  \n",
       "0  /home/christian/ray_results/PPO/PPO_ParaKnapsa...  \n",
       "\n",
       "[1 rows x 48 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_env(config_env):\n",
    "    return ParametricKnapsack()\n",
    "\n",
    "ModelCatalog.register_custom_model(\"kp_param_model\", KPParametricActionsModel)\n",
    "tune.register_env(\"ParaKnapsack-v0\", lambda config: create_env(config))\n",
    "\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "results = tune.run(\n",
    "        \"PPO\",\n",
    "        stop={\"training_iteration\": 10},\n",
    "        config={\n",
    "            \"env\": \"ParaKnapsack-v0\",\n",
    "            \"env_config\": {\n",
    "                \"mask\": False\n",
    "            },\n",
    "            \"model\": {\n",
    "                \"custom_model\": \"kp_param_model\"\n",
    "            },\n",
    "        },\n",
    "        verbose=0,\n",
    "        reuse_actors=True)\n",
    "\n",
    "df = results.dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Parametric Actions to VM Packing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import or_gym\n",
    "from or_gym.algos.rl_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VMPackingEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.cup_capacity = 1\n",
    "        self.mem_capacity = 1\n",
    "        self.t_interval = 20\n",
    "        self.tol = 1e-5\n",
    "        self.step_limit = int(60 * 24 / self.t_interval)\n",
    "        self.n_pms = 50\n",
    "        self.load_idx = np.array([1, 2])\n",
    "        self.seed = 0\n",
    "        self.mask = True\n",
    "        assign_env_config(self, kwargs)\n",
    "        self.action_space = spaces.Discrete(self.n_pms)\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"action_mask\": spaces.Box(0, 1, shape=(self.n_pms,)),\n",
    "            \"avail_actions\": spaces.Box(0, 1, shape=(self.n_pms,)),\n",
    "            \"state\": spaces.Box(0, 1, shape=(self.n_pms+1, 3))\n",
    "        })\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.demand = self.generate_demand()\n",
    "        self.current_step = 0\n",
    "        self.state = {\n",
    "            \"action_mask\": np.ones(self.n_pms),\n",
    "            \"avail_actions\": np.ones(self.n_pms),\n",
    "            \"state\": np.vstack([\n",
    "                np.zeros((self.n_pms, 3)),\n",
    "                self.demand[self.current_step]])\n",
    "        }\n",
    "        self.assignment = {}\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        pm_state = self.state[\"state\"][:-1]\n",
    "        demand = self.state[\"state\"][-1, 1:]\n",
    "        \n",
    "        if action < 0 or action >= self.n_pms:\n",
    "            raise ValueError(\"Invalid action: {}\".format(action))\n",
    "            \n",
    "        elif any(pm_state[action, 1:] + demand > 1 + self.tol):\n",
    "            # Demand doesn't fit into PM\n",
    "            reward = -10000\n",
    "            done = True\n",
    "        else:\n",
    "            if pm_state[action, 0] == 0:\n",
    "                # Open PM if closed\n",
    "                pm_state[action, 0] = 1\n",
    "            pm_state[action, self.load_idx] += demand\n",
    "            reward = np.sum(pm_state[:, 0] * (pm_state[:,1:].sum(axis=1) - 2))\n",
    "            self.assignment[self.current_step] = action\n",
    "            \n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.step_limit:\n",
    "            done = True\n",
    "        self.update_state(pm_state)\n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "    def update_state(self, pm_state):\n",
    "        # Make action selection impossible if the PM would exceed capacity\n",
    "        step = self.current_step if self.current_step < self.step_limit else self.step_limit-1\n",
    "        data_center = np.vstack([pm_state, self.demand[step]])\n",
    "        data_center = np.where(data_center>1,1,data_center) # Fix rounding errors\n",
    "        self.state[\"state\"] = data_center\n",
    "        self.state[\"action_mask\"] = np.ones(self.n_pms)\n",
    "        self.state[\"avail_actions\"] = np.ones(self.n_pms)\n",
    "        if self.mask:\n",
    "            action_mask = (pm_state[:, 1:] + self.demand[step, 1:]) <= 1\n",
    "            self.state[\"action_mask\"] = (action_mask.sum(axis=1)==2).astype(int)\n",
    "                    \n",
    "    def generate_demand(self):\n",
    "        cpu_demand = np.random.uniform(0, 1, size=self.step_limit)\n",
    "        mem_demand = np.random.uniform(0, 1, size=self.step_limit)\n",
    "        return np.vstack([np.zeros(self.step_limit), cpu_demand, mem_demand]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VMParametricActionsModel(TFModelV2):\n",
    "    \n",
    "    def __init__(self, obs_space, action_space, num_outputs,\n",
    "        model_config, name, true_obs_shape=(51,3), action_embed_size=50,\n",
    "        *args, **kwargs):\n",
    "        super(VMParametricActionsModel, self).__init__(obs_space,\n",
    "            action_space, num_outputs, model_config, name, *args, **kwargs)\n",
    "#         print(model_config)\n",
    "        self.action_embed_model = FullyConnectedNetwork(\n",
    "            spaces.Box(0, 1, shape=true_obs_shape), action_space, action_embed_size,\n",
    "            model_config, name + \"_action_embedding\")\n",
    "        self.register_variables(self.action_embed_model.variables())\n",
    "        \n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        avail_actions = input_dict[\"obs\"][\"avail_actions\"]\n",
    "        action_mask = input_dict[\"obs\"][\"action_mask\"]\n",
    "        action_embedding, _ = self.action_embed_model({\n",
    "            \"obs\": input_dict[\"obs\"][\"state\"]\n",
    "        })\n",
    "        intent_vector = tf.expand_dims(action_embedding, 1)\n",
    "        action_logits = tf.reduce_sum(avail_actions * intent_vector, axis=1)\n",
    "        inf_mask = tf.maximum(tf.log(action_mask), tf.float32.min)\n",
    "        return action_logits + inf_mask, state\n",
    "    \n",
    "    def value_function(self):\n",
    "        return self.action_embed_model.value_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VMPackingEnv()\n",
    "state = env.reset()\n",
    "avail_actions = state[\"avail_actions\"]\n",
    "action_mask = state[\"action_mask\"]\n",
    "# action_embed_model = FullyConnectedNetwork(\n",
    "#     spaces.Box(0, 1, shape=env.observation_space[\"state\"].shape),\n",
    "#     action_space=env.action_space.n,\n",
    "#     num_outputs=env.action_space.n,\n",
    "#     model_config={\"custom_model\": \"vm_param_model\"},\n",
    "#     name=\"ParamVMPacking-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = VMParametricActionsModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-22 14:29:59,443\tERROR worker.py:679 -- Calling ray.init() again after it has already been called.\n",
      "2020-04-22 14:29:59,461\tINFO ray_trial_executor.py:121 -- Trial PPO_ParaVMPacking-v0_a7ae71d4: Setting up new remote runner.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/8 CPUs, 0/1 GPUs, 0.0/2.98 GiB heap, 0.0/1.03 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ParaVMPacking-v0_a7ae71d4</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=31543)\u001b[0m 2020-04-22 14:30:01,571\tINFO trainer.py:371 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=31543)\u001b[0m 2020-04-22 14:30:01,574\tINFO trainer.py:512 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=31543)\u001b[0m 2020-04-22 14:30:04,211\tWARNING util.py:45 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=31685)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:151: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(pid=31685)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "\u001b[2m\u001b[36m(pid=31543)\u001b[0m /home/christian/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:151: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(pid=31543)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "Result for PPO_ParaVMPacking-v0_a7ae71d4:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-04-22_14-30-12\n",
      "  done: false\n",
      "  episode_len_mean: 66.31666666666666\n",
      "  episode_reward_max: -1705.7551730784082\n",
      "  episode_reward_mean: -10771.542671799129\n",
      "  episode_reward_min: -11978.651093070603\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 60\n",
      "  experiment_id: 5f6756fd18f1452496355556a6124e24\n",
      "  experiment_tag: '0'\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 4620.911\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.913323402404785\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.1630176454782486\n",
      "        policy_loss: -0.19312149286270142\n",
      "        total_loss: 56721848.0\n",
      "        vf_explained_var: 6.285982817644253e-05\n",
      "        vf_loss: 56721848.0\n",
      "    load_time_ms: 57.245\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 3968\n",
      "    sample_time_ms: 3394.964\n",
      "    update_time_ms: 541.39\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.907692307692315\n",
      "    ram_util_percent: 57.58461538461539\n",
      "  pid: 31543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1496750375498897\n",
      "    mean_inference_ms: 0.6263698714188132\n",
      "    mean_processing_ms: 0.14254940801236343\n",
      "  time_since_restore: 8.659936428070068\n",
      "  time_this_iter_s: 8.659936428070068\n",
      "  time_total_s: 8.659936428070068\n",
      "  timestamp: 1587583812\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: a7ae71d4\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/8 CPUs, 0/1 GPUs, 0.0/2.98 GiB heap, 0.0/1.03 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ParaVMPacking-v0_a7ae71d4</td><td>RUNNING </td><td>192.168.0.11:31543</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.65994</td><td style=\"text-align: right;\">       4000</td><td style=\"text-align: right;\">-10771.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=31543)\u001b[0m 2020-04-22 14:30:12,963\tWARNING ppo.py:144 -- The magnitude of your environment rewards are more than 1077.0x the scale of `vf_clip_param`. This means that it will take more than 1077.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_ParaVMPacking-v0_a7ae71d4:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-04-22_14-30-19\n",
      "  done: false\n",
      "  episode_len_mean: 65.99\n",
      "  episode_reward_max: -1756.725259203345\n",
      "  episode_reward_mean: -10796.289258395978\n",
      "  episode_reward_min: -11978.651093070603\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 120\n",
      "  experiment_id: 5f6756fd18f1452496355556a6124e24\n",
      "  experiment_tag: '0'\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 4550.819\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.9409902095794678\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.14288713037967682\n",
      "        policy_loss: -0.20204399526119232\n",
      "        total_loss: 54315164.0\n",
      "        vf_explained_var: 2.817953827616293e-05\n",
      "        vf_loss: 54315164.0\n",
      "    load_time_ms: 30.653\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 7936\n",
      "    sample_time_ms: 2645.858\n",
      "    update_time_ms: 272.438\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.199999999999996\n",
      "    ram_util_percent: 58.22222222222222\n",
      "  pid: 31543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.15152923920677827\n",
      "    mean_inference_ms: 0.623874465491896\n",
      "    mean_processing_ms: 0.14370461364520135\n",
      "  time_since_restore: 15.049376487731934\n",
      "  time_this_iter_s: 6.389440059661865\n",
      "  time_total_s: 15.049376487731934\n",
      "  timestamp: 1587583819\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: a7ae71d4\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/8 CPUs, 0/1 GPUs, 0.0/2.98 GiB heap, 0.0/1.03 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ParaVMPacking-v0_a7ae71d4</td><td>RUNNING </td><td>192.168.0.11:31543</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         15.0494</td><td style=\"text-align: right;\">       8000</td><td style=\"text-align: right;\">-10796.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=31543)\u001b[0m 2020-04-22 14:30:19,362\tWARNING ppo.py:144 -- The magnitude of your environment rewards are more than 1080.0x the scale of `vf_clip_param`. This means that it will take more than 1080.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_ParaVMPacking-v0_a7ae71d4:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-04-22_14-30-25\n",
      "  done: false\n",
      "  episode_len_mean: 66.07\n",
      "  episode_reward_max: -1720.37181796013\n",
      "  episode_reward_mean: -10788.576038139063\n",
      "  episode_reward_min: -11938.512003571645\n",
      "  episodes_this_iter: 61\n",
      "  episodes_total: 181\n",
      "  experiment_id: 5f6756fd18f1452496355556a6124e24\n",
      "  experiment_tag: '0'\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 4528.968\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.9669272899627686\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.10123495757579803\n",
      "        policy_loss: -0.18695253133773804\n",
      "        total_loss: 55251548.0\n",
      "        vf_explained_var: 1.4285887118603569e-05\n",
      "        vf_loss: 55251548.0\n",
      "    load_time_ms: 21.778\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 11904\n",
      "    sample_time_ms: 2369.961\n",
      "    update_time_ms: 182.69\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.43333333333333\n",
      "    ram_util_percent: 58.166666666666664\n",
      "  pid: 31543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.15313332525616608\n",
      "    mean_inference_ms: 0.620161403423193\n",
      "    mean_processing_ms: 0.143256933679355\n",
      "  time_since_restore: 21.365028381347656\n",
      "  time_this_iter_s: 6.315651893615723\n",
      "  time_total_s: 21.365028381347656\n",
      "  timestamp: 1587583825\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: a7ae71d4\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/8 CPUs, 0/1 GPUs, 0.0/2.98 GiB heap, 0.0/1.03 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ParaVMPacking-v0_a7ae71d4</td><td>RUNNING </td><td>192.168.0.11:31543</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">          21.365</td><td style=\"text-align: right;\">      12000</td><td style=\"text-align: right;\">-10788.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=31543)\u001b[0m 2020-04-22 14:30:25,686\tWARNING ppo.py:144 -- The magnitude of your environment rewards are more than 1079.0x the scale of `vf_clip_param`. This means that it will take more than 1079.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_ParaVMPacking-v0_a7ae71d4:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-04-22_14-30-33\n",
      "  done: false\n",
      "  episode_len_mean: 66.02\n",
      "  episode_reward_max: -1610.1013434851554\n",
      "  episode_reward_mean: -10670.417503275268\n",
      "  episode_reward_min: -11987.581390181427\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 241\n",
      "  experiment_id: 5f6756fd18f1452496355556a6124e24\n",
      "  experiment_tag: '0'\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 4816.979\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.993440866470337\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.07120780646800995\n",
      "        policy_loss: -0.18975572288036346\n",
      "        total_loss: 54589640.0\n",
      "        vf_explained_var: 4.9241125452681445e-06\n",
      "        vf_loss: 54589640.0\n",
      "    load_time_ms: 17.34\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 15872\n",
      "    sample_time_ms: 2243.152\n",
      "    update_time_ms: 137.798\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.04545454545455\n",
      "    ram_util_percent: 58.25454545454545\n",
      "  pid: 31543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.15341725260259587\n",
      "    mean_inference_ms: 0.6166606246004315\n",
      "    mean_processing_ms: 0.14209729791194411\n",
      "  time_since_restore: 28.922929763793945\n",
      "  time_this_iter_s: 7.557901382446289\n",
      "  time_total_s: 28.922929763793945\n",
      "  timestamp: 1587583833\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: a7ae71d4\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/8 CPUs, 0/1 GPUs, 0.0/2.98 GiB heap, 0.0/1.03 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ParaVMPacking-v0_a7ae71d4</td><td>RUNNING </td><td>192.168.0.11:31543</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         28.9229</td><td style=\"text-align: right;\">      16000</td><td style=\"text-align: right;\">-10670.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=31543)\u001b[0m 2020-04-22 14:30:33,253\tWARNING ppo.py:144 -- The magnitude of your environment rewards are more than 1067.0x the scale of `vf_clip_param`. This means that it will take more than 1067.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_ParaVMPacking-v0_a7ae71d4:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-04-22_14-30-41\n",
      "  done: false\n",
      "  episode_len_mean: 66.21\n",
      "  episode_reward_max: -1610.1013434851554\n",
      "  episode_reward_mean: -10793.508658061488\n",
      "  episode_reward_min: -11987.581390181427\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 301\n",
      "  experiment_id: 5f6756fd18f1452496355556a6124e24\n",
      "  experiment_tag: '0'\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 4893.734\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.0297670364379883\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.05637210234999657\n",
      "        policy_loss: -0.20641808211803436\n",
      "        total_loss: 50731368.0\n",
      "        vf_explained_var: 2.584149797257851e-06\n",
      "        vf_loss: 50731368.0\n",
      "    load_time_ms: 14.952\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 19840\n",
      "    sample_time_ms: 2320.442\n",
      "    update_time_ms: 111.365\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.50909090909091\n",
      "    ram_util_percent: 58.218181818181826\n",
      "  pid: 31543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.16165064720325784\n",
      "    mean_inference_ms: 0.6453453367436955\n",
      "    mean_processing_ms: 0.1494989810792141\n",
      "  time_since_restore: 36.770185708999634\n",
      "  time_this_iter_s: 7.8472559452056885\n",
      "  time_total_s: 36.770185708999634\n",
      "  timestamp: 1587583841\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: a7ae71d4\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/8 CPUs, 0/1 GPUs, 0.0/2.98 GiB heap, 0.0/1.03 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ParaVMPacking-v0_a7ae71d4</td><td>RUNNING </td><td>192.168.0.11:31543</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         36.7702</td><td style=\"text-align: right;\">      20000</td><td style=\"text-align: right;\">-10793.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=31543)\u001b[0m 2020-04-22 14:30:41,114\tWARNING ppo.py:144 -- The magnitude of your environment rewards are more than 1079.0x the scale of `vf_clip_param`. This means that it will take more than 1079.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_ParaVMPacking-v0_a7ae71d4:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-04-22_14-30-47\n",
      "  done: false\n",
      "  episode_len_mean: 65.96\n",
      "  episode_reward_max: -1559.7512052288355\n",
      "  episode_reward_mean: -10598.364275083646\n",
      "  episode_reward_min: -12105.00755060112\n",
      "  episodes_this_iter: 61\n",
      "  episodes_total: 362\n",
      "  experiment_id: 5f6756fd18f1452496355556a6124e24\n",
      "  experiment_tag: '0'\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 4864.515\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.0277984142303467\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.04154117777943611\n",
      "        policy_loss: -0.1939602792263031\n",
      "        total_loss: 49093676.0\n",
      "        vf_explained_var: 6.268101628847944e-07\n",
      "        vf_loss: 49093680.0\n",
      "    load_time_ms: 13.306\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 23808\n",
      "    sample_time_ms: 2284.815\n",
      "    update_time_ms: 93.382\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.489999999999995\n",
      "    ram_util_percent: 58.3\n",
      "  pid: 31543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.16821697574077865\n",
      "    mean_inference_ms: 0.6683670093240355\n",
      "    mean_processing_ms: 0.15530376960236528\n",
      "  time_since_restore: 43.609360694885254\n",
      "  time_this_iter_s: 6.83917498588562\n",
      "  time_total_s: 43.609360694885254\n",
      "  timestamp: 1587583847\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: a7ae71d4\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/8 CPUs, 0/1 GPUs, 0.0/2.98 GiB heap, 0.0/1.03 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ParaVMPacking-v0_a7ae71d4</td><td>RUNNING </td><td>192.168.0.11:31543</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         43.6094</td><td style=\"text-align: right;\">      24000</td><td style=\"text-align: right;\">-10598.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=31543)\u001b[0m 2020-04-22 14:30:47,964\tWARNING ppo.py:144 -- The magnitude of your environment rewards are more than 1060.0x the scale of `vf_clip_param`. This means that it will take more than 1060.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_ParaVMPacking-v0_a7ae71d4:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-04-22_14-30-54\n",
      "  done: false\n",
      "  episode_len_mean: 65.39\n",
      "  episode_reward_max: -1587.7718336506312\n",
      "  episode_reward_mean: -10648.717836836688\n",
      "  episode_reward_min: -11878.79930205598\n",
      "  episodes_this_iter: 61\n",
      "  episodes_total: 423\n",
      "  experiment_id: 5f6756fd18f1452496355556a6124e24\n",
      "  experiment_tag: '0'\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 4825.775\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.0529417991638184\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.029261332005262375\n",
      "        policy_loss: -0.18305885791778564\n",
      "        total_loss: 50421188.0\n",
      "        vf_explained_var: 1.0459654049554956e-06\n",
      "        vf_loss: 50421188.0\n",
      "    load_time_ms: 12.021\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 27776\n",
      "    sample_time_ms: 2233.067\n",
      "    update_time_ms: 80.626\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.34444444444445\n",
      "    ram_util_percent: 58.22222222222222\n",
      "  pid: 31543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.16836602494437444\n",
      "    mean_inference_ms: 0.6670874140157156\n",
      "    mean_processing_ms: 0.15512443936915735\n",
      "  time_since_restore: 50.13902497291565\n",
      "  time_this_iter_s: 6.5296642780303955\n",
      "  time_total_s: 50.13902497291565\n",
      "  timestamp: 1587583854\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: a7ae71d4\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/8 CPUs, 0/1 GPUs, 0.0/2.98 GiB heap, 0.0/1.03 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ParaVMPacking-v0_a7ae71d4</td><td>RUNNING </td><td>192.168.0.11:31543</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">          50.139</td><td style=\"text-align: right;\">      28000</td><td style=\"text-align: right;\">-10648.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=31543)\u001b[0m 2020-04-22 14:30:54,504\tWARNING ppo.py:144 -- The magnitude of your environment rewards are more than 1065.0x the scale of `vf_clip_param`. This means that it will take more than 1065.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_ParaVMPacking-v0_a7ae71d4:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-04-22_14-31-01\n",
      "  done: false\n",
      "  episode_len_mean: 65.49\n",
      "  episode_reward_max: -1619.3302662978067\n",
      "  episode_reward_mean: -11347.811343544156\n",
      "  episode_reward_min: -11955.067840410211\n",
      "  episodes_this_iter: 61\n",
      "  episodes_total: 484\n",
      "  experiment_id: 5f6756fd18f1452496355556a6124e24\n",
      "  experiment_tag: '0'\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 4828.401\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.0511744022369385\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018607523292303085\n",
      "        policy_loss: -0.1532767117023468\n",
      "        total_loss: 54535208.0\n",
      "        vf_explained_var: 3.7300972621778783e-07\n",
      "        vf_loss: 54535208.0\n",
      "    load_time_ms: 11.262\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 31744\n",
      "    sample_time_ms: 2202.416\n",
      "    update_time_ms: 70.893\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.230000000000004\n",
      "    ram_util_percent: 58.249999999999986\n",
      "  pid: 31543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1678321501063932\n",
      "    mean_inference_ms: 0.6643325367307803\n",
      "    mean_processing_ms: 0.15444091044474356\n",
      "  time_since_restore: 56.98758411407471\n",
      "  time_this_iter_s: 6.848559141159058\n",
      "  time_total_s: 56.98758411407471\n",
      "  timestamp: 1587583861\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: a7ae71d4\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/8 CPUs, 0/1 GPUs, 0.0/2.98 GiB heap, 0.0/1.03 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ParaVMPacking-v0_a7ae71d4</td><td>RUNNING </td><td>192.168.0.11:31543</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         56.9876</td><td style=\"text-align: right;\">      32000</td><td style=\"text-align: right;\">-11347.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=31543)\u001b[0m 2020-04-22 14:31:01,362\tWARNING ppo.py:144 -- The magnitude of your environment rewards are more than 1135.0x the scale of `vf_clip_param`. This means that it will take more than 1135.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "Result for PPO_ParaVMPacking-v0_a7ae71d4:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-04-22_14-31-07\n",
      "  done: false\n",
      "  episode_len_mean: 65.79\n",
      "  episode_reward_max: -1619.3302662978067\n",
      "  episode_reward_mean: -11158.055945097667\n",
      "  episode_reward_min: -11955.067840410211\n",
      "  episodes_this_iter: 61\n",
      "  episodes_total: 545\n",
      "  experiment_id: 5f6756fd18f1452496355556a6124e24\n",
      "  experiment_tag: '0'\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 4797.406\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.0295326709747314\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017420435324311256\n",
      "        policy_loss: -0.14462466537952423\n",
      "        total_loss: 56178296.0\n",
      "        vf_explained_var: 2.1919127846103947e-07\n",
      "        vf_loss: 56178300.0\n",
      "    load_time_ms: 10.504\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 35712\n",
      "    sample_time_ms: 2161.726\n",
      "    update_time_ms: 63.345\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.63333333333334\n",
      "    ram_util_percent: 58.29999999999999\n",
      "  pid: 31543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.16679857900293701\n",
      "    mean_inference_ms: 0.6598309923922883\n",
      "    mean_processing_ms: 0.15338454632004836\n",
      "  time_since_restore: 63.38550806045532\n",
      "  time_this_iter_s: 6.397923946380615\n",
      "  time_total_s: 63.38550806045532\n",
      "  timestamp: 1587583867\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: a7ae71d4\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=31543)\u001b[0m 2020-04-22 14:31:07,769\tWARNING ppo.py:144 -- The magnitude of your environment rewards are more than 1116.0x the scale of `vf_clip_param`. This means that it will take more than 1116.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/8 CPUs, 0/1 GPUs, 0.0/2.98 GiB heap, 0.0/1.03 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ParaVMPacking-v0_a7ae71d4</td><td>RUNNING </td><td>192.168.0.11:31543</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         63.3855</td><td style=\"text-align: right;\">      36000</td><td style=\"text-align: right;\">-11158.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ParaVMPacking-v0_a7ae71d4:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-04-22_14-31-14\n",
      "  done: true\n",
      "  episode_len_mean: 65.54\n",
      "  episode_reward_max: -1626.8245643665698\n",
      "  episode_reward_mean: -10864.878687447072\n",
      "  episode_reward_min: -11927.848078843032\n",
      "  episodes_this_iter: 61\n",
      "  episodes_total: 606\n",
      "  experiment_id: 5f6756fd18f1452496355556a6124e24\n",
      "  experiment_tag: '0'\n",
      "  hostname: ubuntu\n",
      "  info:\n",
      "    grad_time_ms: 4776.208\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.043523073196411\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02020074799656868\n",
      "        policy_loss: -0.1774006485939026\n",
      "        total_loss: 47420836.0\n",
      "        vf_explained_var: 6.345010916675164e-08\n",
      "        vf_loss: 47420836.0\n",
      "    load_time_ms: 9.882\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 39680\n",
      "    sample_time_ms: 2145.642\n",
      "    update_time_ms: 57.315\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.11\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.11999999999999\n",
      "    ram_util_percent: 58.36999999999999\n",
      "  pid: 31543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.16625206623684313\n",
      "    mean_inference_ms: 0.6574822185782365\n",
      "    mean_processing_ms: 0.15289694515084937\n",
      "  time_since_restore: 69.98400735855103\n",
      "  time_this_iter_s: 6.598499298095703\n",
      "  time_total_s: 69.98400735855103\n",
      "  timestamp: 1587583874\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: a7ae71d4\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/2.98 GiB heap, 0.0/1.03 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ParaVMPacking-v0_a7ae71d4</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">          69.984</td><td style=\"text-align: right;\">      40000</td><td style=\"text-align: right;\">-10864.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/2.98 GiB heap, 0.0/1.03 GiB objects<br>Result logdir: /home/christian/ray_results/PPO<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  timesteps</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ParaVMPacking-v0_a7ae71d4</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">          69.984</td><td style=\"text-align: right;\">      40000</td><td style=\"text-align: right;\">-10864.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-22 14:31:14,395\tINFO tune.py:334 -- Returning an analysis object by default. You can call `analysis.trials` to retrieve a list of trials. This message will be removed in future versions of Tune.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=31543)\u001b[0m 2020-04-22 14:31:14,376\tWARNING ppo.py:144 -- The magnitude of your environment rewards are more than 1086.0x the scale of `vf_clip_param`. This means that it will take more than 1086.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>timesteps_this_iter</th>\n",
       "      <th>done</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>...</th>\n",
       "      <th>info/learner/default_policy/policy_loss</th>\n",
       "      <th>info/learner/default_policy/vf_loss</th>\n",
       "      <th>info/learner/default_policy/vf_explained_var</th>\n",
       "      <th>info/learner/default_policy/kl</th>\n",
       "      <th>info/learner/default_policy/entropy</th>\n",
       "      <th>info/learner/default_policy/entropy_coeff</th>\n",
       "      <th>config/env</th>\n",
       "      <th>config/env_config</th>\n",
       "      <th>config/model</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1626.824564</td>\n",
       "      <td>-11927.848079</td>\n",
       "      <td>-10864.878687</td>\n",
       "      <td>65.54</td>\n",
       "      <td>61</td>\n",
       "      <td>4000</td>\n",
       "      <td>True</td>\n",
       "      <td>40000</td>\n",
       "      <td>606</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177401</td>\n",
       "      <td>47420836.0</td>\n",
       "      <td>6.345011e-08</td>\n",
       "      <td>0.020201</td>\n",
       "      <td>3.043523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ParaVMPacking-v0</td>\n",
       "      <td>{'mask': True}</td>\n",
       "      <td>{'custom_model': 'vm_param_model'}</td>\n",
       "      <td>/home/christian/ray_results/PPO/PPO_ParaVMPack...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "0        -1626.824564       -11927.848079        -10864.878687   \n",
       "\n",
       "   episode_len_mean  episodes_this_iter  timesteps_this_iter  done  \\\n",
       "0             65.54                  61                 4000  True   \n",
       "\n",
       "   timesteps_total  episodes_total  training_iteration  ...  \\\n",
       "0            40000             606                  10  ...   \n",
       "\n",
       "  info/learner/default_policy/policy_loss info/learner/default_policy/vf_loss  \\\n",
       "0                               -0.177401                          47420836.0   \n",
       "\n",
       "   info/learner/default_policy/vf_explained_var  \\\n",
       "0                                  6.345011e-08   \n",
       "\n",
       "   info/learner/default_policy/kl  info/learner/default_policy/entropy  \\\n",
       "0                        0.020201                             3.043523   \n",
       "\n",
       "   info/learner/default_policy/entropy_coeff        config/env  \\\n",
       "0                                        0.0  ParaVMPacking-v0   \n",
       "\n",
       "  config/env_config                        config/model  \\\n",
       "0    {'mask': True}  {'custom_model': 'vm_param_model'}   \n",
       "\n",
       "                                              logdir  \n",
       "0  /home/christian/ray_results/PPO/PPO_ParaVMPack...  \n",
       "\n",
       "[1 rows x 48 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = try_import_tf()\n",
    "\n",
    "def create_env(config_env):\n",
    "    return VMPackingEnv()\n",
    "\n",
    "ModelCatalog.register_custom_model(\"vm_param_model\", VMParametricActionsModel)\n",
    "tune.register_env(\"ParaVMPacking-v0\", lambda config: create_env(config))\n",
    "\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "results = tune.run(\n",
    "        \"PPO\",\n",
    "        stop={\"training_iteration\": 10},\n",
    "        config={\n",
    "            \"env\": \"ParaVMPacking-v0\",\n",
    "            \"env_config\": {\n",
    "                \"mask\": True\n",
    "            },\n",
    "            \"model\": {\n",
    "                \"custom_model\": \"vm_param_model\"\n",
    "            },\n",
    "        },\n",
    "#         verbose=0,\n",
    "        reuse_actors=True)\n",
    "\n",
    "df = results.dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "56/15000*3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
