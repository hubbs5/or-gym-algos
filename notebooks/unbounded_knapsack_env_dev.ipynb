{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from pyomo.environ import *\n",
    "from pyomo.opt import SolverFactory\n",
    "import or_gym\n",
    "%matplotlib inlinelearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Knapsack-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnapsackEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.item_weights = [1, 2, 3, 6, 10, 18]\n",
    "        self.item_values = [0, 1, 3, 14, 20, 100]\n",
    "        self.item_numbers = np.arange(len(self.item_weights))\n",
    "        self.N = len(self.item_weights)\n",
    "        self.max_weight = 15\n",
    "        self.current_weight = 0\n",
    "        \n",
    "        self.action_space = spaces.Discrete(self.N)\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(self.N),\n",
    "            spaces.Discrete(self.N),\n",
    "            spaces.Discrete(1)))\n",
    "        \n",
    "        self.seed()\n",
    "        self.reset()\n",
    "        \n",
    "    def step(self, item):\n",
    "        # Check that item will fit\n",
    "        if self.item_weights[item] + self.current_weight <= self.max_weight:\n",
    "            self.current_weight += self.item_weights[item]\n",
    "            reward = self.item_values[item]\n",
    "            if self.current_weight == self.max_weight:\n",
    "                done = True\n",
    "            else:\n",
    "                done = False\n",
    "        else:\n",
    "            # End trial if over weight\n",
    "            reward = 0\n",
    "            done = True\n",
    "            \n",
    "        self._update_state()\n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        return self.state\n",
    "    \n",
    "    def _update_state(self):\n",
    "        self.state = (self.item_weights,\n",
    "                      self.item_values, \n",
    "                      self.current_weight)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_weight = 0\n",
    "        self._update_state()\n",
    "        return self.state\n",
    "    \n",
    "    def sample_action(self):\n",
    "        return np.random.choice(self.item_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(([1, 2, 3, 6, 10, 18], [0, 1, 3, 14, 20, 100], 3), 3, False, {})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = KnapsackEnv()\n",
    "env.step(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve KP with DP\n",
    "\n",
    "We can test this model by solving it with dynamic programming approaches.\n",
    "\n",
    "Thikning about this carefully, we see that, for the simple dynamic programming approach to the unbounded problem, there are only as many states as the carrying capacity divided by the smallest weight. In the simple case above, we have a max capacity of 15, and the smallest item we have has a weight of 1. This gives us 15 states. This becomes much more complicated when we have a bounded problem such that the state is defined by the weight and the remaining number of items to pack, so we'll leave that for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "Value iteration was easy to solve for and determine the values. So let's move on to policy iteration to get a better understanding of how this works.\n",
    "\n",
    "We start with a random policy where all actions at each state are equally probable. We then alternate between evaluating the policy and improving the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, gamma):\n",
    "    # Value iteration\n",
    "    v = np.zeros(env.max_weight + 1)\n",
    "\n",
    "    # Terminal state values are 0\n",
    "    delta = 1e-5\n",
    "    delta_t = 1\n",
    "    k = 0\n",
    "    while delta_t > delta:\n",
    "        v_new = np.zeros(v.shape)\n",
    "        for i in range(v_new.shape[0]):\n",
    "            # Check for terminal state\n",
    "            if i == env.max_weight:\n",
    "                continue\n",
    "            else:\n",
    "                for action in env.item_numbers:\n",
    "                    if i + env.item_weights[action] <= env.max_weight:\n",
    "                        s_1 = i + env.item_weights[action]\n",
    "                        r = env.item_values[action]\n",
    "                    else:\n",
    "                        s_1 = env.N\n",
    "                        r = 0\n",
    "                    prob = policy[i, action]\n",
    "                    v_new[i] += prob * (r + gamma * v[s_1])\n",
    "\n",
    "        k += 1\n",
    "        delta_t = np.sum(np.abs(v - v_new))\n",
    "        v = v_new.copy()\n",
    "    return v\n",
    "\n",
    "def policy_improvement(env, policy, values, gamma):\n",
    "    new_policy = np.zeros(policy.shape)\n",
    "    \n",
    "    for i in range(new_policy.shape[0]):\n",
    "        for j in range(new_policy.shape[1]):\n",
    "            # Get the action values\n",
    "            action_vals = []\n",
    "            for action in env.item_numbers:\n",
    "                # Update state and take step\n",
    "                env.current_weight = i\n",
    "                env._get_obs()\n",
    "                s_1, r, done, _ = env.step(action)\n",
    "                val = r + gamma * (values[i])\n",
    "                action_vals.append([val, action])\n",
    "                \n",
    "            # Select the best action\n",
    "            action_vals = np.array(action_vals)\n",
    "            best_actions = np.argwhere(\n",
    "                action_vals[:,0]==np.max(action_vals[:, 0])).flatten()\n",
    "            # Randomize in cases where multiple maximums exist\n",
    "            if len(best_actions) > 1:\n",
    "                best_actions = np.random.choice(best_actions)\n",
    "            new_policy[i, best_actions] = 1\n",
    "\n",
    "    num_policy_changes = np.sum(policy != new_policy).astype(int)\n",
    "    return new_policy, num_policy_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_policy(env, policy):\n",
    "    state = env.reset()[-1]\n",
    "    done = False\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    while done == False:\n",
    "        action = np.argmax(policy[state])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        state = next_state[-1]\n",
    "    return np.array(rewards), np.array(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 3 iterations\n"
     ]
    }
   ],
   "source": [
    "# Initialize Policy\n",
    "policy = np.ones((env.max_weight + 1, env.N)) * 1 / env.N\n",
    "env.reset()\n",
    "gamma = 1\n",
    "k = 0\n",
    "reward_deque = deque(maxlen=3)\n",
    "action_deque = deque(maxlen=3)\n",
    "while True:\n",
    "    values = policy_evaluation(env, policy, gamma)\n",
    "    policy, num_changes = policy_improvement(env, policy, values, gamma)\n",
    "    k += 1\n",
    "    if num_changes == 0:\n",
    "        break\n",
    "    # After so long, it looks like these policies are only updating\n",
    "    # at the very end where it doesn't matter. We'll play the env\n",
    "    # with the policies to avoid an infinite loop. If 3 in a row are\n",
    "    # identical, then we'll call it quits\n",
    "    rewards, actions = play_policy(env, policy)\n",
    "    reward_deque.append(rewards)\n",
    "    action_deque.append(actions)\n",
    "    if len(reward_deque) == 3:\n",
    "        converge_r = all([np.allclose(reward_deque[i-1], reward_deque[i])\n",
    "            for i, j in enumerate(reward_deque)])\n",
    "        converge_a = all([np.allclose(action_deque[i-1], action_deque[i])\n",
    "            for i, j in enumerate(action_deque)])\n",
    "        if converge_r and converge_a:\n",
    "            print(\"Converged after {} iterations\".format(k))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_deque[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MILP of Model\n",
    "\n",
    "$$\\textrm{max} \\; z = \\sum_i v_i x_i$$\n",
    "\n",
    "$$\\textrm{s.t.} \\; \\sum_i w_i x_i \\leq W$$\n",
    "\n",
    "$$x_i \\in [0, 1]$$\n",
    "$$v_i, w_i \\in \\rm I\\!R^+$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "m = ConcreteModel()\n",
    "\n",
    "# Sets, parameters, and variables\n",
    "m.W = env.max_weight\n",
    "m.i = Set(initialize=env.item_numbers)\n",
    "m.w = Param(m.i, \n",
    "    initialize={i: j for i, j in zip(env.item_numbers, env.item_weights)})\n",
    "m.v = Param(m.i, \n",
    "    initialize={i: j for i, j in zip(env.item_numbers, env.item_values)})\n",
    "m.x = Var(m.i, within=Binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@m.Constraint(m.i)\n",
    "def weight_constraint(m, i):\n",
    "    return sum(m.w[i] * m.x[i] for i in m.i) - m.W <= 0\n",
    "\n",
    "m.obj = Objective(expr=(\n",
    "    sum([m.v[i] * m.x[i] for i in m.i])),\n",
    "    sense=maximize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLPSOL: GLPK LP/MIP Solver, v4.65\n",
      "Parameter(s) specified in the command line:\n",
      " --write C:\\Users\\U755275\\AppData\\Local\\Temp\\tmpfe5eyz0f.glpk.raw --wglp C:\\Users\\U755275\\AppData\\Local\\Temp\\tmpxtl8b7lk.glpk.glp\n",
      " --cpxlp C:\\Users\\U755275\\AppData\\Local\\Temp\\tmpn9s7hjk2.pyomo.lp\n",
      "Reading problem data from 'C:\\Users\\U755275\\AppData\\Local\\Temp\\tmpn9s7hjk2.pyomo.lp'...\n",
      "C:\\Users\\U755275\\AppData\\Local\\Temp\\tmpn9s7hjk2.pyomo.lp:78: warning: lower bound of variable 'x1' redefined\n",
      "C:\\Users\\U755275\\AppData\\Local\\Temp\\tmpn9s7hjk2.pyomo.lp:78: warning: upper bound of variable 'x1' redefined\n",
      "7 rows, 7 columns, 37 non-zeros\n",
      "6 integer variables, all of which are binary\n",
      "84 lines were read\n",
      "Writing problem data to 'C:\\Users\\U755275\\AppData\\Local\\Temp\\tmpxtl8b7lk.glpk.glp'...\n",
      "67 lines were written\n",
      "GLPK Integer Optimizer, v4.65\n",
      "7 rows, 7 columns, 37 non-zeros\n",
      "6 integer variables, all of which are binary\n",
      "Preprocessing...\n",
      "6 constraint coefficient(s) were reduced\n",
      "6 rows, 5 columns, 30 non-zeros\n",
      "5 integer variables, all of which are binary\n",
      "Scaling...\n",
      " A: min|aij| =  1.000e+00  max|aij| =  7.000e+00  ratio =  7.000e+00\n",
      "Problem data seem to be well scaled\n",
      "Constructing initial basis...\n",
      "Size of triangular part is 6\n",
      "Solving LP relaxation...\n",
      "GLPK Simplex Optimizer, v4.65\n",
      "6 rows, 5 columns, 30 non-zeros\n",
      "*     0: obj =  -0.000000000e+00 inf =   0.000e+00 (4)\n",
      "*     2: obj =   3.166666667e+01 inf =   0.000e+00 (0)\n",
      "OPTIMAL LP SOLUTION FOUND\n",
      "Integer optimization begins...\n",
      "Long-step dual simplex will be used\n",
      "+     2: mip =     not found yet <=              +inf        (1; 0)\n",
      "+     3: >>>>>   1.800000000e+01 <=   2.500000000e+01  38.9% (2; 0)\n",
      "+     4: >>>>>   2.400000000e+01 <=   2.400000000e+01   0.0% (1; 1)\n",
      "+     4: mip =   2.400000000e+01 <=     tree is empty   0.0% (0; 3)\n",
      "INTEGER OPTIMAL SOLUTION FOUND\n",
      "Time used:   0.0 secs\n",
      "Memory used: 0.1 Mb (65497 bytes)\n",
      "Writing MIP solution to 'C:\\Users\\U755275\\AppData\\Local\\Temp\\tmpfe5eyz0f.glpk.raw'...\n",
      "23 lines were written\n"
     ]
    }
   ],
   "source": [
    "solver = SolverFactory('glpk')\n",
    "results = solver.solve(m, tee=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.obj.expr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 4]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in m.i if m.x[i]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
